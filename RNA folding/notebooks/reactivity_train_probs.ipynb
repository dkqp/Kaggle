{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA Reactivity Training\n",
    "https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../data/small_sets/train_data_QUICK_START.csv'\n",
    "TRAIN_DATA_EXT_PATH = '../data/train_extracted.csv'\n",
    "VOCAB_PATH = '../data/vocab.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>reactivity_0004</th>\n",
       "      <th>reactivity_0005</th>\n",
       "      <th>reactivity_0006</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_error_0197</th>\n",
       "      <th>reactivity_error_0198</th>\n",
       "      <th>reactivity_error_0199</th>\n",
       "      <th>reactivity_error_0200</th>\n",
       "      <th>reactivity_error_0201</th>\n",
       "      <th>reactivity_error_0202</th>\n",
       "      <th>reactivity_error_0203</th>\n",
       "      <th>reactivity_error_0204</th>\n",
       "      <th>reactivity_error_0205</th>\n",
       "      <th>reactivity_error_0206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021f968267</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_Replicates_from_previous_l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence_id                                           sequence  \\\n",
       "0  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "1  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "2  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "3  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "4  00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
       "\n",
       "  experiment_type                                       dataset_name  \\\n",
       "0         2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n",
       "1         DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n",
       "2         2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n",
       "3         DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n",
       "4         2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n",
       "\n",
       "   reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
       "0              NaN              NaN              NaN              NaN   \n",
       "1              NaN              NaN              NaN              NaN   \n",
       "2              NaN              NaN              NaN              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              NaN   \n",
       "\n",
       "   reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n",
       "0              NaN              NaN  ...                    NaN   \n",
       "1              NaN              NaN  ...                    NaN   \n",
       "2              NaN              NaN  ...                    NaN   \n",
       "3              NaN              NaN  ...                    NaN   \n",
       "4              NaN              NaN  ...                    NaN   \n",
       "\n",
       "   reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n",
       "0                    NaN                    NaN                    NaN  \n",
       "1                    NaN                    NaN                    NaN  \n",
       "2                    NaN                    NaN                    NaN  \n",
       "3                    NaN                    NaN                    NaN  \n",
       "4                    NaN                    NaN                    NaN  \n",
       "\n",
       "[5 rows x 416 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_pd = pd.read_csv(TRAIN_DATA_PATH)\n",
    "train_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_ext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>.....((((((.....)))))).....((((((((((((((....)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>.....((((((.....))))))........(((((..(.....).....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...</td>\n",
       "      <td>.....((((((.....))))))........(((((.((((.........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGAGAUCGAAGACGACUUAC...</td>\n",
       "      <td>.....((((((.....))))))....((((((((.....(.........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGACUGACGAAGUCG...</td>\n",
       "      <td>.....((((((.....))))))....(((..(((((((((..((((...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "1  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "2  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
       "3  GGGAACGACUCGAGUAGAGUCGAAAAGGAGAUCGAAGACGACUUAC...   \n",
       "4  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGACUGACGAAGUCG...   \n",
       "\n",
       "                                        sequence_ext  \n",
       "0  .....((((((.....)))))).....((((((((((((((....)...  \n",
       "1  .....((((((.....))))))........(((((..(.....).....  \n",
       "2  .....((((((.....))))))........(((((.((((.........  \n",
       "3  .....((((((.....))))))....((((((((.....(.........  \n",
       "4  .....((((((.....))))))....(((..(((((((((..((((...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_extracted_pd = pd.read_csv(TRAIN_DATA_EXT_PATH)\n",
    "train_extracted_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 167808)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_pd), len(train_extracted_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "root_dir = '/kaggle/input/stanford-ribonanza-rna-folding/Ribonanza_bpp_files/extra_data'\n",
    "prob_file_paths = []\n",
    "for forder, _, files in tqdm(os.walk(root_dir), total=len(os.listdir(root_dir))):\n",
    "    for file in files:\n",
    "        prob_file_paths.append(os.path.join(forder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sequence_ids = set(train_data_pd['sequence_id'])\n",
    "prob_file_paths_set = set(prob_file_paths)\n",
    "\n",
    "path_probs_dict = {}\n",
    "\n",
    "for i in tqdm(range(len(prob_file_paths))):\n",
    "    file_name = os.path.splitext(os.path.basename(prob_file_paths[i]))[0]\n",
    "    if file_name in unique_sequence_ids:\n",
    "        path_probs_dict[file_name] = prob_file_paths[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.dataset import RNADataset_train\n",
    "\n",
    "\n",
    "rna_dataset = RNADataset_train(\n",
    "    data=train_data_pd[:2000],\n",
    "    data_ext = train_extracted_pd[:1000],\n",
    "    vocab=pd.read_csv(VOCAB_PATH),\n",
    "    max_len=210\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rna_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([210]), torch.Size([2, 210, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rna_dataset[0][0].shape, rna_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BERTCustomRNAReactivity                            --\n",
       "├─BERTCustom: 1-1                                  --\n",
       "│    └─CombEmbedding: 2-1                          --\n",
       "│    │    └─TokenEmbedding: 3-1                    11,776\n",
       "│    │    └─PositionEmbedding: 3-2                 --\n",
       "│    │    └─Dropout: 3-3                           --\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─EncoderBlock: 3-4                      3,152,384\n",
       "│    │    └─EncoderBlock: 3-5                      3,152,384\n",
       "│    │    └─EncoderBlock: 3-6                      3,152,384\n",
       "│    │    └─EncoderBlock: 3-7                      3,152,384\n",
       "│    │    └─EncoderBlock: 3-8                      3,152,384\n",
       "│    │    └─EncoderBlock: 3-9                      3,152,384\n",
       "│    │    └─EncoderBlock: 3-10                     3,152,384\n",
       "│    │    └─EncoderBlock: 3-11                     3,152,384\n",
       "│    │    └─EncoderBlock: 3-12                     3,152,384\n",
       "│    │    └─EncoderBlock: 3-13                     3,152,384\n",
       "│    │    └─EncoderBlock: 3-14                     3,152,384\n",
       "│    │    └─EncoderBlock: 3-15                     3,152,384\n",
       "├─Linear: 1-2                                      4,104\n",
       "===========================================================================\n",
       "Total params: 37,844,488\n",
       "Trainable params: 37,844,488\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.model import BERTCustomRNAReactivity, BERTCustom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bertmodel = BERTCustom(\n",
    "    vocab_size=len(rna_dataset.vocab),\n",
    "    hidden=512,\n",
    "    dim_k=64,\n",
    "    num_layer=12,\n",
    "    num_attn_head=8\n",
    ")\n",
    "RNA_model = BERTCustomRNAReactivity(bertmodel)\n",
    "\n",
    "summary(RNA_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 210])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 210, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNA_model(next(iter(DataLoader(rna_dataset, 3)))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "bertmodel.load_state_dict(torch.load('./lightning_logs/version_0/checkpoints/pretrained_bert.pt'))\n",
    "\n",
    "for param in bertmodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# for param in bertmodel.encoder_blocks[11].parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bertmodel.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type                    | Params\n",
      "--------------------------------------------------\n",
      "0 | model | BERTCustomRNAReactivity | 37.8 M\n",
      "--------------------------------------------------\n",
      "37.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "37.8 M    Total params\n",
      "151.378   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:370: UserWarning: You have overridden `on_before_batch_transfer` in `LightningModule` but have passed in a `LightningDataModule`. It will use the implementation from `LightningModule` instance.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 210])\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:02<00:02,  2.27s/it]torch.Size([8, 210])\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/100 [00:00<?, ?it/s] torch.Size([8, 420])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/100 [00:03<05:41,  3.45s/it, v_num=4, train_loss=0.573]torch.Size([8, 236])\n",
      "Epoch 0:   2%|▏         | 2/100 [00:03<03:14,  1.98s/it, v_num=4, train_loss=0.565]torch.Size([8, 358])\n",
      "Epoch 0:   3%|▎         | 3/100 [00:04<02:30,  1.55s/it, v_num=4, train_loss=0.584]torch.Size([8, 420])\n",
      "Epoch 0:   4%|▍         | 4/100 [00:05<02:06,  1.32s/it, v_num=4, train_loss=0.575]torch.Size([8, 320])\n",
      "Epoch 0:   5%|▌         | 5/100 [00:05<01:52,  1.19s/it, v_num=4, train_loss=0.515]torch.Size([8, 325])\n",
      "Epoch 0:   6%|▌         | 6/100 [00:06<01:44,  1.11s/it, v_num=4, train_loss=0.527]torch.Size([8, 420])\n",
      "Epoch 0:   7%|▋         | 7/100 [00:07<01:34,  1.02s/it, v_num=4, train_loss=0.500]torch.Size([8, 332])\n",
      "Epoch 0:   8%|▊         | 8/100 [00:07<01:27,  1.05it/s, v_num=4, train_loss=0.494]torch.Size([8, 410])\n",
      "Epoch 0:   9%|▉         | 9/100 [00:08<01:23,  1.09it/s, v_num=4, train_loss=0.480]torch.Size([8, 358])\n",
      "Epoch 0:  10%|█         | 10/100 [00:08<01:19,  1.14it/s, v_num=4, train_loss=0.489]torch.Size([8, 405])\n",
      "Epoch 0:  11%|█         | 11/100 [00:09<01:16,  1.17it/s, v_num=4, train_loss=0.502]torch.Size([8, 276])\n",
      "Epoch 0:  12%|█▏        | 12/100 [00:09<01:12,  1.22it/s, v_num=4, train_loss=0.522]torch.Size([8, 353])\n",
      "Epoch 0:  13%|█▎        | 13/100 [00:10<01:10,  1.24it/s, v_num=4, train_loss=0.523]torch.Size([8, 352])\n",
      "Epoch 0:  14%|█▍        | 14/100 [00:11<01:07,  1.27it/s, v_num=4, train_loss=0.539]torch.Size([8, 420])\n",
      "Epoch 0:  15%|█▌        | 15/100 [00:11<01:06,  1.28it/s, v_num=4, train_loss=0.526]torch.Size([8, 247])\n",
      "Epoch 0:  16%|█▌        | 16/100 [00:12<01:03,  1.31it/s, v_num=4, train_loss=0.502]torch.Size([8, 420])\n",
      "Epoch 0:  17%|█▋        | 17/100 [00:12<01:02,  1.33it/s, v_num=4, train_loss=0.479]torch.Size([8, 390])\n",
      "Epoch 0:  18%|█▊        | 18/100 [00:13<01:00,  1.35it/s, v_num=4, train_loss=0.447]torch.Size([8, 394])\n",
      "Epoch 0:  19%|█▉        | 19/100 [00:13<00:59,  1.36it/s, v_num=4, train_loss=0.455]torch.Size([8, 361])\n",
      "Epoch 0:  20%|██        | 20/100 [00:14<00:58,  1.37it/s, v_num=4, train_loss=0.477]torch.Size([8, 223])\n",
      "Epoch 0:  21%|██        | 21/100 [00:15<00:56,  1.40it/s, v_num=4, train_loss=0.468]torch.Size([8, 420])\n",
      "Epoch 0:  22%|██▏       | 22/100 [00:15<00:55,  1.41it/s, v_num=4, train_loss=0.479]torch.Size([8, 323])\n",
      "Epoch 0:  23%|██▎       | 23/100 [00:16<00:54,  1.43it/s, v_num=4, train_loss=0.465]torch.Size([8, 240])\n",
      "Epoch 0:  24%|██▍       | 24/100 [00:16<00:52,  1.43it/s, v_num=4, train_loss=0.401]torch.Size([8, 420])\n",
      "Epoch 0:  25%|██▌       | 25/100 [00:17<00:51,  1.45it/s, v_num=4, train_loss=0.389]torch.Size([8, 420])\n",
      "Epoch 0:  26%|██▌       | 26/100 [00:17<00:50,  1.46it/s, v_num=4, train_loss=0.407]torch.Size([8, 350])\n",
      "Epoch 0:  27%|██▋       | 27/100 [00:18<00:49,  1.47it/s, v_num=4, train_loss=0.422]torch.Size([8, 328])\n",
      "Epoch 0:  28%|██▊       | 28/100 [00:18<00:48,  1.48it/s, v_num=4, train_loss=0.391]torch.Size([8, 417])\n",
      "Epoch 0:  29%|██▉       | 29/100 [00:19<00:48,  1.47it/s, v_num=4, train_loss=0.378]torch.Size([8, 291])\n",
      "Epoch 0:  30%|███       | 30/100 [00:20<00:47,  1.48it/s, v_num=4, train_loss=0.354]torch.Size([8, 376])\n",
      "Epoch 0:  31%|███       | 31/100 [00:20<00:46,  1.49it/s, v_num=4, train_loss=0.360]torch.Size([8, 304])\n",
      "Epoch 0:  32%|███▏      | 32/100 [00:21<00:45,  1.50it/s, v_num=4, train_loss=0.388]torch.Size([8, 261])\n",
      "Epoch 0:  33%|███▎      | 33/100 [00:21<00:44,  1.51it/s, v_num=4, train_loss=0.346]torch.Size([8, 274])\n",
      "Epoch 0:  34%|███▍      | 34/100 [00:22<00:43,  1.52it/s, v_num=4, train_loss=0.345]torch.Size([8, 220])\n",
      "Epoch 0:  35%|███▌      | 35/100 [00:23<00:42,  1.52it/s, v_num=4, train_loss=0.330]torch.Size([8, 390])\n",
      "Epoch 0:  36%|███▌      | 36/100 [00:23<00:41,  1.54it/s, v_num=4, train_loss=0.361]torch.Size([8, 420])\n",
      "Epoch 0:  37%|███▋      | 37/100 [00:24<00:40,  1.54it/s, v_num=4, train_loss=0.341]torch.Size([8, 358])\n",
      "Epoch 0:  38%|███▊      | 38/100 [00:24<00:39,  1.55it/s, v_num=4, train_loss=0.314]torch.Size([8, 420])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/13 [00:00<?, ?it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:   8%|▊         | 1/13 [00:00<00:00, 16.65it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  15%|█▌        | 2/13 [00:00<00:00, 15.52it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  23%|██▎       | 3/13 [00:00<00:00, 10.10it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  31%|███       | 4/13 [00:00<00:00, 11.29it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  38%|███▊      | 5/13 [00:00<00:00, 12.30it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  46%|████▌     | 6/13 [00:00<00:00, 12.89it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  54%|█████▍    | 7/13 [00:00<00:00, 13.22it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  62%|██████▏   | 8/13 [00:00<00:00, 13.39it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  69%|██████▉   | 9/13 [00:00<00:00, 13.46it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  77%|███████▋  | 10/13 [00:00<00:00, 13.51it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  85%|████████▍ | 11/13 [00:00<00:00, 13.53it/s]torch.Size([8, 210])\n",
      "Testing DataLoader 0:  92%|█████████▏| 12/13 [00:00<00:00, 13.55it/s]torch.Size([4, 210])\n",
      "Testing DataLoader 0: 100%|██████████| 13/13 [00:01<00:00,  9.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2497769296169281     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2497769296169281    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.2497769296169281}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.dataset import RNADataModule\n",
    "from python_scripts.transformers.task import RNATask\n",
    "\n",
    "rna_datamodule = RNADataModule(whole_train_dataset=rna_dataset, batch_size=8, augmentation=True)\n",
    "\n",
    "def rna_rmse_loss(x: torch.tensor, y: torch.tensor, ignore_index=-100):\n",
    "    not_ignore = y != ignore_index\n",
    "    return torch.sqrt(torch.square(x[not_ignore] - y[not_ignore]).mean())\n",
    "\n",
    "def rna_mse_loss(x: torch.tensor, y: torch.tensor, ignore_index=-100):\n",
    "    not_ignore = y != ignore_index\n",
    "    return torch.square(x[not_ignore] - y[not_ignore]).mean()\n",
    "\n",
    "def rna_mae_loss(x: torch.tensor, y: torch.tensor, ignore_index=-100):\n",
    "    not_ignore = y != ignore_index\n",
    "    return torch.abs(x[not_ignore] - y[not_ignore]).mean()\n",
    "\n",
    "rna_optimizer = torch.optim.Adam(RNA_model.parameters(), 1e-3)\n",
    "# rna_optimizer = torch.optim.SGD(RNA_model.parameters(), 1e-3, 0.9)\n",
    "# rna_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     rna_optimizer,\n",
    "#     T_max=5,\n",
    "#     eta_min=1e-4,\n",
    "#     verbose=True,\n",
    "# )\n",
    "# rna_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     rna_optimizer,\n",
    "#     [4, 7, 10, 13, 16, 19],\n",
    "#     verbose=True,\n",
    "#     gamma=0.3\n",
    "# )\n",
    "# rna_scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "#     optimizer=rna_optimizer,\n",
    "#     base_lr=1e-6,\n",
    "#     max_lr=1e-3,\n",
    "#     step_size_up=3000,\n",
    "#     step_size_down=7000,\n",
    "#     verbose=True\n",
    "# )\n",
    "rna_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=rna_optimizer,\n",
    "    max_lr=1e-4,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=5,\n",
    "    div_factor=1e2,\n",
    "    pct_start=0.1,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "rna_task = RNATask(\n",
    "    model=RNA_model,\n",
    "    loss_fn=rna_mae_loss,\n",
    "    optimizer=rna_optimizer,\n",
    "    scheduler=rna_scheduler,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor='val_avg_loss',\n",
    "    save_top_k=3,\n",
    "    mode='min'\n",
    "))\n",
    "# callbacks.append(EarlyStopping(\n",
    "#     monitor='val_avg_loss',\n",
    "#     min_delta=0.001,\n",
    "#     patience=3,\n",
    "#     verbose=True,\n",
    "#     mode='min'\n",
    "# ))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# rna_task = RNATask.load_from_checkpoint(\n",
    "#     checkpoint_path='./lightning_log/~~'\n",
    "#     model=RNA_model,\n",
    "#     loss_fn=rna_mae_loss,\n",
    "#     optimizer=rna_optimizer,\n",
    "#     scheduler=rna_scheduler,\n",
    "# )\n",
    "\n",
    "# trainer.fit(rna_task, ckpt_path=\"some/path/to/my_checkpoint.ckpt\")# trainer = pl.Trainer(resume_from_checkpoint='../notebooks/lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt')\n",
    "\n",
    "trainer.fit(rna_task, datamodule=rna_datamodule)\n",
    "trainer.test(rna_task, datamodule=rna_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
