{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA folding prediction\n",
    "https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../data/RNA folding/train_data_QUICK_START.csv'\n",
    "TEST_DATA_PATH = '../data/RNA folding/test_sequences.csv'\n",
    "SUBMISSION_FILE_PATH = '../data/RNA folding/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_pd = pd.read_csv(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>reactivity_0004</th>\n",
       "      <th>reactivity_0005</th>\n",
       "      <th>reactivity_0006</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_error_0197</th>\n",
       "      <th>reactivity_error_0198</th>\n",
       "      <th>reactivity_error_0199</th>\n",
       "      <th>reactivity_error_0200</th>\n",
       "      <th>reactivity_error_0201</th>\n",
       "      <th>reactivity_error_0202</th>\n",
       "      <th>reactivity_error_0203</th>\n",
       "      <th>reactivity_error_0204</th>\n",
       "      <th>reactivity_error_0205</th>\n",
       "      <th>reactivity_error_0206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021f968267</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_Replicates_from_previous_l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence_id                                           sequence  \\\n",
       "0  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "1  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "2  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "3  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "4  00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
       "\n",
       "  experiment_type                                       dataset_name  \\\n",
       "0         2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n",
       "1         DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n",
       "2         2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n",
       "3         DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n",
       "4         2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n",
       "\n",
       "   reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
       "0              NaN              NaN              NaN              NaN   \n",
       "1              NaN              NaN              NaN              NaN   \n",
       "2              NaN              NaN              NaN              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              NaN   \n",
       "\n",
       "   reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n",
       "0              NaN              NaN  ...                    NaN   \n",
       "1              NaN              NaN  ...                    NaN   \n",
       "2              NaN              NaN  ...                    NaN   \n",
       "3              NaN              NaN  ...                    NaN   \n",
       "4              NaN              NaN  ...                    NaN   \n",
       "\n",
       "   reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n",
       "0                    NaN                    NaN                    NaN  \n",
       "1                    NaN                    NaN                    NaN  \n",
       "2                    NaN                    NaN                    NaN  \n",
       "3                    NaN                    NaN                    NaN  \n",
       "4                    NaN                    NaN                    NaN  \n",
       "\n",
       "[5 rows x 416 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from python_scripts.transformers.dataset import MaskedDataset, RNADataset\n",
    "\n",
    "masked_dataset = MaskedDataset(\n",
    "    data=train_data_pd.iloc[:1000]['sequence'],\n",
    "    vocab=pd.read_csv('../data/RNA folding/vocab.csv'),\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "label = []\n",
    "for i in range(1, 207):\n",
    "    n = 4 - len(str(i))\n",
    "    label.append(train_data_pd[f\"reactivity_{'0' * n + str(i)}\"])\n",
    "rna_dataset = RNADataset(\n",
    "    data=train_data_pd['sequence'],\n",
    "    label=np.array(label).transpose((1, 0)),\n",
    "    vocab=pd.read_csv('../data/RNA folding/vocab.csv'),\n",
    "    max_len=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masked_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BERTCustomMasked                                   --\n",
       "├─BERTCustom: 1-1                                  --\n",
       "│    └─CombEmbedding: 2-1                          --\n",
       "│    │    └─TokenEmbedding: 3-1                    2,304\n",
       "│    │    └─PositionEmbedding: 3-2                 --\n",
       "│    │    └─Dropout: 3-3                           --\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─EncoderBlock: 3-4                      921,216\n",
       "│    │    └─EncoderBlock: 3-5                      921,216\n",
       "│    │    └─EncoderBlock: 3-6                      921,216\n",
       "│    │    └─EncoderBlock: 3-7                      921,216\n",
       "│    │    └─EncoderBlock: 3-8                      921,216\n",
       "│    │    └─EncoderBlock: 3-9                      921,216\n",
       "│    │    └─EncoderBlock: 3-10                     921,216\n",
       "│    │    └─EncoderBlock: 3-11                     921,216\n",
       "│    │    └─EncoderBlock: 3-12                     921,216\n",
       "│    │    └─EncoderBlock: 3-13                     921,216\n",
       "│    │    └─EncoderBlock: 3-14                     921,216\n",
       "│    │    └─EncoderBlock: 3-15                     921,216\n",
       "├─Linear: 1-2                                      2,313\n",
       "===========================================================================\n",
       "Total params: 11,059,209\n",
       "Trainable params: 11,059,209\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.model import BERTCustomMasked, BERTCustom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bertmodel = BERTCustom(\n",
    "    vocab_size=len(masked_dataset.vocab),\n",
    "    hidden=256,\n",
    "    dim_k=32,\n",
    ")\n",
    "model = BERTCustomMasked(bertmodel)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return torch.tensor(batch)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 9])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(DataLoader(masked_dataset, 3, collate_fn=collate_fn)))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model by masking tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/core/optimizer.py:360: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | model   | BERTCustomMasked   | 11.1 M\n",
      "1 | loss_fn | CrossEntropyLoss   | 0     \n",
      "2 | acc_fn  | MulticlassAccuracy | 0     \n",
      "-----------------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.237    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:02<00:02,  2.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/torchmetrics/functional/classification/accuracy.py:65: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  tp = tp.sum(dim=0 if multidim_average == \"global\" else 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 50/50 [00:39<00:00,  1.26it/s, v_num=1, train_loss=0.0433, train_accuracy=0.312]{'train_avg_loss': tensor(0.0472, device='mps:0'), 'train_avg_accuracy': tensor(0.2844, device='mps:0'), 'val_avg_loss': tensor(0.0506, device='mps:0'), 'val_avg_accuracy': tensor(0.2801, device='mps:0')}\n",
      "\n",
      "Epoch 0, Avg. Training Loss: 0.047 Avg. Training Accuracy: 0.284 Avg. Validation Loss: 0.051 Avg. Validation Accuracy: 0.280\n",
      "Epoch 1: 100%|██████████| 50/50 [00:40<00:00,  1.24it/s, v_num=1, train_loss=0.0438, train_accuracy=0.331, val_loss=0.0439, val_accuracy=0.319]{'train_avg_loss': tensor(0.0439, device='mps:0'), 'train_avg_accuracy': tensor(0.3194, device='mps:0'), 'val_avg_loss': tensor(0.0437, device='mps:0'), 'val_avg_accuracy': tensor(0.3139, device='mps:0')}\n",
      "\n",
      "Epoch 1, Avg. Training Loss: 0.044 Avg. Training Accuracy: 0.319 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.314\n",
      "Epoch 2: 100%|██████████| 50/50 [00:40<00:00,  1.24it/s, v_num=1, train_loss=0.0444, train_accuracy=0.223, val_loss=0.0437, val_accuracy=0.317]{'train_avg_loss': tensor(0.0437, device='mps:0'), 'train_avg_accuracy': tensor(0.3120, device='mps:0'), 'val_avg_loss': tensor(0.0446, device='mps:0'), 'val_avg_accuracy': tensor(0.2412, device='mps:0')}\n",
      "\n",
      "Epoch 2, Avg. Training Loss: 0.044 Avg. Training Accuracy: 0.312 Avg. Validation Loss: 0.045 Avg. Validation Accuracy: 0.241\n",
      "Epoch 3: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, v_num=1, train_loss=0.0445, train_accuracy=0.302, val_loss=0.0443, val_accuracy=0.249]{'train_avg_loss': tensor(0.0437, device='mps:0'), 'train_avg_accuracy': tensor(0.3168, device='mps:0'), 'val_avg_loss': tensor(0.0435, device='mps:0'), 'val_avg_accuracy': tensor(0.3446, device='mps:0')}\n",
      "\n",
      "Epoch 3, Avg. Training Loss: 0.044 Avg. Training Accuracy: 0.317 Avg. Validation Loss: 0.043 Avg. Validation Accuracy: 0.345\n",
      "Epoch 4: 100%|██████████| 50/50 [00:38<00:00,  1.31it/s, v_num=1, train_loss=0.0442, train_accuracy=0.291, val_loss=0.0434, val_accuracy=0.339]{'train_avg_loss': tensor(0.0434, device='mps:0'), 'train_avg_accuracy': tensor(0.3312, device='mps:0'), 'val_avg_loss': tensor(0.0438, device='mps:0'), 'val_avg_accuracy': tensor(0.3079, device='mps:0')}\n",
      "\n",
      "Epoch 4, Avg. Training Loss: 0.043 Avg. Training Accuracy: 0.331 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.308\n",
      "Epoch 5: 100%|██████████| 50/50 [00:38<00:00,  1.31it/s, v_num=1, train_loss=0.0433, train_accuracy=0.326, val_loss=0.0438, val_accuracy=0.316]{'train_avg_loss': tensor(0.0434, device='mps:0'), 'train_avg_accuracy': tensor(0.3332, device='mps:0'), 'val_avg_loss': tensor(0.0436, device='mps:0'), 'val_avg_accuracy': tensor(0.3429, device='mps:0')}\n",
      "\n",
      "Epoch 5, Avg. Training Loss: 0.043 Avg. Training Accuracy: 0.333 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.343\n",
      "Epoch 6: 100%|██████████| 50/50 [00:37<00:00,  1.34it/s, v_num=1, train_loss=0.0439, train_accuracy=0.293, val_loss=0.0437, val_accuracy=0.338]{'train_avg_loss': tensor(0.0434, device='mps:0'), 'train_avg_accuracy': tensor(0.3340, device='mps:0'), 'val_avg_loss': tensor(0.0437, device='mps:0'), 'val_avg_accuracy': tensor(0.3308, device='mps:0')}\n",
      "\n",
      "Epoch 6, Avg. Training Loss: 0.043 Avg. Training Accuracy: 0.334 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.331\n",
      "Epoch 7: 100%|██████████| 50/50 [00:38<00:00,  1.30it/s, v_num=1, train_loss=0.0424, train_accuracy=0.377, val_loss=0.0437, val_accuracy=0.332]{'train_avg_loss': tensor(0.0432, device='mps:0'), 'train_avg_accuracy': tensor(0.3306, device='mps:0'), 'val_avg_loss': tensor(0.0434, device='mps:0'), 'val_avg_accuracy': tensor(0.3521, device='mps:0')}\n",
      "\n",
      "Epoch 7, Avg. Training Loss: 0.043 Avg. Training Accuracy: 0.331 Avg. Validation Loss: 0.043 Avg. Validation Accuracy: 0.352\n",
      "Epoch 8: 100%|██████████| 50/50 [00:38<00:00,  1.30it/s, v_num=1, train_loss=0.0426, train_accuracy=0.354, val_loss=0.0437, val_accuracy=0.353]{'train_avg_loss': tensor(0.0434, device='mps:0'), 'train_avg_accuracy': tensor(0.3273, device='mps:0'), 'val_avg_loss': tensor(0.0440, device='mps:0'), 'val_avg_accuracy': tensor(0.3368, device='mps:0')}\n",
      "\n",
      "Epoch 8, Avg. Training Loss: 0.043 Avg. Training Accuracy: 0.327 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.337\n",
      "Epoch 9: 100%|██████████| 50/50 [00:37<00:00,  1.32it/s, v_num=1, train_loss=0.0455, train_accuracy=0.321, val_loss=0.0439, val_accuracy=0.337]{'train_avg_loss': tensor(0.0434, device='mps:0'), 'train_avg_accuracy': tensor(0.3313, device='mps:0'), 'val_avg_loss': tensor(0.0436, device='mps:0'), 'val_avg_accuracy': tensor(0.3518, device='mps:0')}\n",
      "\n",
      "Epoch 9, Avg. Training Loss: 0.043 Avg. Training Accuracy: 0.331 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.352\n",
      "Epoch 9: 100%|██████████| 50/50 [00:39<00:00,  1.27it/s, v_num=1, train_loss=0.0455, train_accuracy=0.321, val_loss=0.0438, val_accuracy=0.342]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 50/50 [00:39<00:00,  1.27it/s, v_num=1, train_loss=0.0455, train_accuracy=0.321, val_loss=0.0438, val_accuracy=0.342]\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.dataset import MaskedDataModule\n",
    "from python_scripts.transformers.task import MaskingTask\n",
    "\n",
    "masked_datamodule = MaskedDataModule(masked_dataset, batch_size=16)\n",
    "\n",
    "maskingtask = MaskingTask(\n",
    "    model=model,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), 1e-3),\n",
    "    scheduler=torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "    acc_fn=Accuracy(task='multiclass', num_classes=len(masked_dataset.vocab), ignore_index=-100)\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    ")\n",
    "\n",
    "trainer.fit(maskingtask, datamodule=masked_datamodule)\n",
    "trainer.test(maskingtask, datamodule=masked_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
