{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA folding prediction\n",
    "https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../data/RNA folding/train_data_QUICK_START.csv'\n",
    "TEST_DATA_PATH = '../data/RNA folding/test_sequences.csv'\n",
    "SUBMISSION_FILE_PATH = '../data/RNA folding/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_pd = pd.read_csv(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>reactivity_0004</th>\n",
       "      <th>reactivity_0005</th>\n",
       "      <th>reactivity_0006</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_error_0197</th>\n",
       "      <th>reactivity_error_0198</th>\n",
       "      <th>reactivity_error_0199</th>\n",
       "      <th>reactivity_error_0200</th>\n",
       "      <th>reactivity_error_0201</th>\n",
       "      <th>reactivity_error_0202</th>\n",
       "      <th>reactivity_error_0203</th>\n",
       "      <th>reactivity_error_0204</th>\n",
       "      <th>reactivity_error_0205</th>\n",
       "      <th>reactivity_error_0206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021f968267</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_Replicates_from_previous_l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence_id                                           sequence  \\\n",
       "0  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "1  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "2  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "3  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "4  00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
       "\n",
       "  experiment_type                                       dataset_name  \\\n",
       "0         2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n",
       "1         DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n",
       "2         2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n",
       "3         DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n",
       "4         2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n",
       "\n",
       "   reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
       "0              NaN              NaN              NaN              NaN   \n",
       "1              NaN              NaN              NaN              NaN   \n",
       "2              NaN              NaN              NaN              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              NaN   \n",
       "\n",
       "   reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n",
       "0              NaN              NaN  ...                    NaN   \n",
       "1              NaN              NaN  ...                    NaN   \n",
       "2              NaN              NaN  ...                    NaN   \n",
       "3              NaN              NaN  ...                    NaN   \n",
       "4              NaN              NaN  ...                    NaN   \n",
       "\n",
       "   reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n",
       "0                    NaN                    NaN                    NaN  \n",
       "1                    NaN                    NaN                    NaN  \n",
       "2                    NaN                    NaN                    NaN  \n",
       "3                    NaN                    NaN                    NaN  \n",
       "4                    NaN                    NaN                    NaN  \n",
       "\n",
       "[5 rows x 416 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from python_scripts.transformers.dataset import MaskedDataset, RNADataset\n",
    "\n",
    "masked_dataset = MaskedDataset(\n",
    "    data=train_data_pd[:1000],\n",
    "    vocab=pd.read_csv('../data/RNA folding/vocab.csv'),\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "rna_dataset = RNADataset(\n",
    "    data=train_data_pd[:1000],\n",
    "    vocab=pd.read_csv('../data/RNA folding/vocab.csv'),\n",
    "    max_len=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masked_dataset), len(rna_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BERTCustomMasked                                   --\n",
       "├─BERTCustom: 1-1                                  --\n",
       "│    └─CombEmbedding: 2-1                          --\n",
       "│    │    └─TokenEmbedding: 3-1                    2,816\n",
       "│    │    └─PositionEmbedding: 3-2                 --\n",
       "│    │    └─Dropout: 3-3                           --\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─EncoderBlock: 3-4                      921,216\n",
       "│    │    └─EncoderBlock: 3-5                      921,216\n",
       "│    │    └─EncoderBlock: 3-6                      921,216\n",
       "│    │    └─EncoderBlock: 3-7                      921,216\n",
       "│    │    └─EncoderBlock: 3-8                      921,216\n",
       "│    │    └─EncoderBlock: 3-9                      921,216\n",
       "│    │    └─EncoderBlock: 3-10                     921,216\n",
       "│    │    └─EncoderBlock: 3-11                     921,216\n",
       "│    │    └─EncoderBlock: 3-12                     921,216\n",
       "│    │    └─EncoderBlock: 3-13                     921,216\n",
       "│    │    └─EncoderBlock: 3-14                     921,216\n",
       "│    │    └─EncoderBlock: 3-15                     921,216\n",
       "├─Linear: 1-2                                      2,827\n",
       "===========================================================================\n",
       "Total params: 11,060,235\n",
       "Trainable params: 11,060,235\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.model import BERTCustomMasked, BERTCustom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bertmodel = BERTCustom(\n",
    "    vocab_size=len(masked_dataset.vocab),\n",
    "    hidden=256,\n",
    "    dim_k=32,\n",
    ")\n",
    "masked_model = BERTCustomMasked(bertmodel)\n",
    "\n",
    "summary(masked_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 11])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_model(next(iter(DataLoader(masked_dataset, 3)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BERTCustomRNAReactivity                            --\n",
       "├─BERTCustom: 1-1                                  --\n",
       "│    └─CombEmbedding: 2-1                          --\n",
       "│    │    └─TokenEmbedding: 3-1                    2,816\n",
       "│    │    └─PositionEmbedding: 3-2                 --\n",
       "│    │    └─Dropout: 3-3                           --\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─EncoderBlock: 3-4                      921,216\n",
       "│    │    └─EncoderBlock: 3-5                      921,216\n",
       "│    │    └─EncoderBlock: 3-6                      921,216\n",
       "│    │    └─EncoderBlock: 3-7                      921,216\n",
       "│    │    └─EncoderBlock: 3-8                      921,216\n",
       "│    │    └─EncoderBlock: 3-9                      921,216\n",
       "│    │    └─EncoderBlock: 3-10                     921,216\n",
       "│    │    └─EncoderBlock: 3-11                     921,216\n",
       "│    │    └─EncoderBlock: 3-12                     921,216\n",
       "│    │    └─EncoderBlock: 3-13                     921,216\n",
       "│    │    └─EncoderBlock: 3-14                     921,216\n",
       "│    │    └─EncoderBlock: 3-15                     921,216\n",
       "├─Linear: 1-2                                      257\n",
       "├─Sigmoid: 1-3                                     --\n",
       "===========================================================================\n",
       "Total params: 11,057,665\n",
       "Trainable params: 11,057,665\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.model import BERTCustomRNAReactivity, BERTCustom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bertmodel = BERTCustom(\n",
    "    vocab_size=len(masked_dataset.vocab),\n",
    "    hidden=256,\n",
    "    dim_k=32,\n",
    ")\n",
    "RNA_model = BERTCustomRNAReactivity(bertmodel)\n",
    "\n",
    "summary(RNA_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNA_model(next(iter(DataLoader(rna_dataset, 3)))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DataLoader(rna_dataset, 3)))[0].dtype, next(iter(DataLoader(rna_dataset, 3)))[1].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model by masking tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | model   | BERTCustomMasked   | 11.1 M\n",
      "1 | loss_fn | CrossEntropyLoss   | 0     \n",
      "2 | acc_fn  | MulticlassAccuracy | 0     \n",
      "-----------------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.241    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/torchmetrics/functional/classification/accuracy.py:65: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  tp = tp.sum(dim=0 if multidim_average == \"global\" else 1)\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 99/100 [00:40<00:00,  2.47it/s, v_num=7, train_loss=0.0441, train_accuracy=0.341]Adjusting learning rate of group 0 to 9.0451e-04.\n",
      "Epoch 0: 100%|██████████| 100/100 [00:40<00:00,  2.47it/s, v_num=7, train_loss=0.0441, train_accuracy=0.323]\n",
      "Epoch 0, Avg. Training Loss: 0.047 Avg. Training Accuracy: 0.301 Avg. Validation Loss: 0.049 Avg. Validation Accuracy: 0.280\n",
      "Epoch 1:  99%|█████████▉| 99/100 [00:41<00:00,  2.41it/s, v_num=7, train_loss=0.044, train_accuracy=0.333, val_loss=0.0446, val_accuracy=0.313]  Adjusting learning rate of group 0 to 6.5451e-04.\n",
      "Epoch 1: 100%|██████████| 100/100 [00:41<00:00,  2.41it/s, v_num=7, train_loss=0.0428, train_accuracy=0.422, val_loss=0.0446, val_accuracy=0.313]\n",
      "Epoch 1, Avg. Training Loss: 0.045 Avg. Training Accuracy: 0.314 Avg. Validation Loss: 0.045 Avg. Validation Accuracy: 0.325\n",
      "Epoch 2:  99%|█████████▉| 99/100 [00:41<00:00,  2.39it/s, v_num=7, train_loss=0.0448, train_accuracy=0.318, val_loss=0.0447, val_accuracy=0.322] Adjusting learning rate of group 0 to 3.4549e-04.\n",
      "Epoch 2: 100%|██████████| 100/100 [00:41<00:00,  2.39it/s, v_num=7, train_loss=0.0436, train_accuracy=0.302, val_loss=0.0447, val_accuracy=0.322]\n",
      "Epoch 2, Avg. Training Loss: 0.044 Avg. Training Accuracy: 0.309 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.315\n",
      "Epoch 3:  99%|█████████▉| 99/100 [00:40<00:00,  2.43it/s, v_num=7, train_loss=0.0444, train_accuracy=0.346, val_loss=0.0442, val_accuracy=0.318] Adjusting learning rate of group 0 to 9.5492e-05.\n",
      "Epoch 3: 100%|██████████| 100/100 [00:41<00:00,  2.43it/s, v_num=7, train_loss=0.0466, train_accuracy=0.354, val_loss=0.0442, val_accuracy=0.318]\n",
      "Epoch 3, Avg. Training Loss: 0.044 Avg. Training Accuracy: 0.321 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.301\n",
      "Epoch 3: 100%|██████████| 100/100 [00:42<00:00,  2.35it/s, v_num=7, train_loss=0.0466, train_accuracy=0.354, val_loss=0.0444, val_accuracy=0.300]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 13/13 [00:01<00:00,  9.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.323074072599411     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.04438428953289986    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.323074072599411    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04438428953289986   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.04438428953289986, 'test_accuracy': 0.323074072599411}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.dataset import MaskedDataModule\n",
    "from python_scripts.transformers.task import MaskingTask\n",
    "\n",
    "masked_datamodule = MaskedDataModule(masked_dataset, batch_size=8)\n",
    "\n",
    "masked_optimizer = torch.optim.Adam(masked_model.parameters(), 1e-3)\n",
    "masked_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    masked_optimizer,\n",
    "    T_max=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "maskingtask = MaskingTask(\n",
    "    model=masked_model,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=masked_optimizer,\n",
    "    scheduler=masked_scheduler,\n",
    "    acc_fn=Accuracy(task='multiclass', num_classes=len(masked_dataset.vocab), ignore_index=-100)\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor='val_avg_accuracy',\n",
    "    save_top_k=2,\n",
    "    mode='max'\n",
    "))\n",
    "callbacks.append(EarlyStopping(\n",
    "    monitor='val_avg_accuracy',\n",
    "    min_delta=0.1,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='max'\n",
    "))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# maskingtask = MaskingTask.load_from_checkpoint(\n",
    "#     './lightning_logs/version_0/checkpoints/epoch=0-step=33562.ckpt',\n",
    "#     model=masked_model,\n",
    "#     loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "#     optimizer=masked_optimizer,\n",
    "#     scheduler=masked_scheduler,\n",
    "#     acc_fn=Accuracy(task='multiclass', num_classes=len(masked_dataset.vocab), ignore_index=-100)\n",
    "# )\n",
    "\n",
    "trainer.fit(maskingtask, datamodule=masked_datamodule)\n",
    "trainer.test(maskingtask, datamodule=masked_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type                    | Params\n",
      "--------------------------------------------------\n",
      "0 | model | BERTCustomRNAReactivity | 11.1 M\n",
      "--------------------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.231    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:370: UserWarning: You have overridden `on_before_batch_transfer` in `LightningModule` but have passed in a `LightningDataModule`. It will use the implementation from `LightningModule` instance.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:34<00:00,  2.88it/s, v_num=26, train_loss=90.00]\n",
      "Epoch 0, Avg. Training Loss: 90.084 Avg. Validation Loss: 90.197\n",
      "Epoch 1: 100%|██████████| 100/100 [00:35<00:00,  2.84it/s, v_num=26, train_loss=90.10, val_loss=90.20]\n",
      "Epoch 1, Avg. Training Loss: 90.084 Avg. Validation Loss: 90.189\n",
      "Epoch 2: 100%|██████████| 100/100 [00:36<00:00,  2.77it/s, v_num=26, train_loss=90.00, val_loss=90.20]\n",
      "Epoch 2, Avg. Training Loss: 90.084 Avg. Validation Loss: 90.189\n",
      "Epoch 3: 100%|██████████| 100/100 [00:36<00:00,  2.78it/s, v_num=26, train_loss=90.50, val_loss=90.20]\n",
      "Epoch 3, Avg. Training Loss: 90.083 Avg. Validation Loss: 90.189\n",
      "Epoch 3: 100%|██████████| 100/100 [00:37<00:00,  2.67it/s, v_num=26, train_loss=90.50, val_loss=90.20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 13/13 [00:01<00:00, 10.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     90.1828384399414      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    90.1828384399414     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 90.1828384399414}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.dataset import RNADataModule\n",
    "from python_scripts.transformers.task import RNATask\n",
    "\n",
    "rna_datamodule = RNADataModule(rna_dataset, batch_size=8)\n",
    "\n",
    "rna_optimizer = torch.optim.Adam(masked_model.parameters(), 1e-3)\n",
    "# rna_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     rna_optimizer,\n",
    "#     T_max=5,\n",
    "#     verbose=True,\n",
    "# )\n",
    "rna_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    rna_optimizer,\n",
    "    [3, 6, 9]\n",
    ")\n",
    "\n",
    "rna_task = RNATask(\n",
    "    model=RNA_model,\n",
    "    loss_fn=lambda x, y: torch.sqrt(torch.nn.MSELoss()(x, y)),\n",
    "    optimizer=rna_optimizer,\n",
    "    scheduler=rna_scheduler,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor='val_avg_loss',\n",
    "    save_top_k=2,\n",
    "    mode='min'\n",
    "))\n",
    "callbacks.append(EarlyStopping(\n",
    "    monitor='val_avg_loss',\n",
    "    min_delta=0.1,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='min'\n",
    "))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# rna_task = RNATask.load_from_checkpoint(\n",
    "#     './lightning_logs/version_0/checkpoints/epoch=0-step=33562.ckpt',\n",
    "#     model=RNA_model,\n",
    "#     loss_fn=torch.nn.MSELoss(),\n",
    "#     optimizer=rna_optimizer,\n",
    "#     scheduler=rna_scheduler,\n",
    "# )\n",
    "\n",
    "trainer.fit(rna_task, datamodule=rna_datamodule)\n",
    "trainer.test(rna_task, datamodule=rna_datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
