{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA folding prediction\n",
    "https://www.kaggle.com/competitions/stanford-ribonanza-rna-folding/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = '../data/RNA folding/train_data_QUICK_START.csv'\n",
    "TRAIN_DATA_EXT_PATH = '../data/RNA folding/train_extracted.csv'\n",
    "TEST_DATA_PATH = '../data/RNA folding/test_sequences.csv'\n",
    "TEST_DATA_EXT_PATH = '../data/RNA folding/test_extracted.csv'\n",
    "SUBMISSION_SAMPLE_FILE_PATH = '../data/RNA folding/sample_submission.csv'\n",
    "SUBMISSION_FILE_PATH = '../data/RNA folding/submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>reactivity_0004</th>\n",
       "      <th>reactivity_0005</th>\n",
       "      <th>reactivity_0006</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_error_0197</th>\n",
       "      <th>reactivity_error_0198</th>\n",
       "      <th>reactivity_error_0199</th>\n",
       "      <th>reactivity_error_0200</th>\n",
       "      <th>reactivity_error_0201</th>\n",
       "      <th>reactivity_error_0202</th>\n",
       "      <th>reactivity_error_0203</th>\n",
       "      <th>reactivity_error_0204</th>\n",
       "      <th>reactivity_error_0205</th>\n",
       "      <th>reactivity_error_0206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000d87cab97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_RFAM_windows_100mers_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001ca9d21b0</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>DMS_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021f968267</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>DasLabBigLib_OneMil_Replicates_from_previous_l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence_id                                           sequence  \\\n",
       "0  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "1  0000d87cab97  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "2  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "3  0001ca9d21b0  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "4  00021f968267  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
       "\n",
       "  experiment_type                                       dataset_name  \\\n",
       "0         2A3_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_2A3   \n",
       "1         DMS_MaP       DasLabBigLib_OneMil_RFAM_windows_100mers_DMS   \n",
       "2         2A3_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_2A3   \n",
       "3         DMS_MaP     DasLabBigLib_OneMil_OpenKnot_Round_2_train_DMS   \n",
       "4         2A3_MaP  DasLabBigLib_OneMil_Replicates_from_previous_l...   \n",
       "\n",
       "   reactivity_0001  reactivity_0002  reactivity_0003  reactivity_0004  \\\n",
       "0              NaN              NaN              NaN              NaN   \n",
       "1              NaN              NaN              NaN              NaN   \n",
       "2              NaN              NaN              NaN              NaN   \n",
       "3              NaN              NaN              NaN              NaN   \n",
       "4              NaN              NaN              NaN              NaN   \n",
       "\n",
       "   reactivity_0005  reactivity_0006  ...  reactivity_error_0197  \\\n",
       "0              NaN              NaN  ...                    NaN   \n",
       "1              NaN              NaN  ...                    NaN   \n",
       "2              NaN              NaN  ...                    NaN   \n",
       "3              NaN              NaN  ...                    NaN   \n",
       "4              NaN              NaN  ...                    NaN   \n",
       "\n",
       "   reactivity_error_0198  reactivity_error_0199  reactivity_error_0200  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0201  reactivity_error_0202  reactivity_error_0203  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0204  reactivity_error_0205  reactivity_error_0206  \n",
       "0                    NaN                    NaN                    NaN  \n",
       "1                    NaN                    NaN                    NaN  \n",
       "2                    NaN                    NaN                    NaN  \n",
       "3                    NaN                    NaN                    NaN  \n",
       "4                    NaN                    NaN                    NaN  \n",
       "\n",
       "[5 rows x 416 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data_pd = pd.read_csv(TRAIN_DATA_PATH)\n",
    "train_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_ext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...</td>\n",
       "      <td>.....((((((.....)))))).....((((((((((((((....)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...</td>\n",
       "      <td>.....((((((.....))))))........(((((..(.....).....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...</td>\n",
       "      <td>.....((((((.....))))))........(((((.((((.........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGGAGAUCGAAGACGACUUAC...</td>\n",
       "      <td>.....((((((.....))))))....((((((((.....(.........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGACUGACGAAGUCG...</td>\n",
       "      <td>.....((((((.....))))))....(((..(((((((((..((((...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  GGGAACGACUCGAGUAGAGUCGAAAAAGAUCGCCACGCACUUACGA...   \n",
       "1  GGGAACGACUCGAGUAGAGUCGAAAAGGUGGCCGGCAGAAUCGCGA...   \n",
       "2  GGGAACGACUCGAGUAGAGUCGAAAACAUUGUUAAUGCCUAUAUUA...   \n",
       "3  GGGAACGACUCGAGUAGAGUCGAAAAGGAGAUCGAAGACGACUUAC...   \n",
       "4  GGGAACGACUCGAGUAGAGUCGAAAAGAUAUGGACUGACGAAGUCG...   \n",
       "\n",
       "                                        sequence_ext  \n",
       "0  .....((((((.....)))))).....((((((((((((((....)...  \n",
       "1  .....((((((.....))))))........(((((..(.....).....  \n",
       "2  .....((((((.....))))))........(((((.((((.........  \n",
       "3  .....((((((.....))))))....((((((((.....(.........  \n",
       "4  .....((((((.....))))))....(((..(((((((((..((((...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_extracted_pd = pd.read_csv(TRAIN_DATA_EXT_PATH)\n",
    "train_extracted_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from python_scripts.transformers.dataset import MaskedDataset, RNADataset_2\n",
    "\n",
    "masked_dataset = MaskedDataset(\n",
    "    data=train_data_pd[:1000],\n",
    "    vocab=pd.read_csv('../data/RNA folding/vocab.csv'),\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "rna_dataset = RNADataset_2(\n",
    "    data=train_data_pd[:2000],\n",
    "    data_ext = train_extracted_pd[:1000],\n",
    "    vocab=pd.read_csv('../data/RNA folding/vocab.csv'),\n",
    "    max_len=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(masked_dataset), len(rna_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.model import BERTCustomMasked, BERTCustom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bertmodel = BERTCustom(\n",
    "    vocab_size=len(masked_dataset.vocab),\n",
    "    hidden=32,\n",
    "    dim_k=4,\n",
    ")\n",
    "masked_model = BERTCustomMasked(bertmodel)\n",
    "\n",
    "summary(masked_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 23])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_model(next(iter(DataLoader(masked_dataset, 3)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "BERTCustomRNAReactivity_2                          --\n",
       "├─BERTCustom: 1-1                                  --\n",
       "│    └─CombEmbedding: 2-1                          --\n",
       "│    │    └─TokenEmbedding: 3-1                    2,944\n",
       "│    │    └─PositionEmbedding: 3-2                 --\n",
       "│    │    └─Dropout: 3-3                           --\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─EncoderBlock: 3-4                      231,232\n",
       "│    │    └─EncoderBlock: 3-5                      231,232\n",
       "│    │    └─EncoderBlock: 3-6                      231,232\n",
       "│    │    └─EncoderBlock: 3-7                      231,232\n",
       "│    │    └─EncoderBlock: 3-8                      231,232\n",
       "│    │    └─EncoderBlock: 3-9                      231,232\n",
       "│    │    └─EncoderBlock: 3-10                     231,232\n",
       "│    │    └─EncoderBlock: 3-11                     231,232\n",
       "│    │    └─EncoderBlock: 3-12                     231,232\n",
       "│    │    └─EncoderBlock: 3-13                     231,232\n",
       "│    │    └─EncoderBlock: 3-14                     231,232\n",
       "│    │    └─EncoderBlock: 3-15                     231,232\n",
       "├─Linear: 1-2                                      258\n",
       "===========================================================================\n",
       "Total params: 2,777,986\n",
       "Trainable params: 2,777,986\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.model import BERTCustomRNAReactivity_2, BERTCustom\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bertmodel = BERTCustom(\n",
    "    vocab_size=len(masked_dataset.vocab),\n",
    "    hidden=128,\n",
    "    dim_k=16,\n",
    "    num_layer=12,\n",
    "    num_attn_head=12\n",
    ")\n",
    "RNA_model = BERTCustomRNAReactivity_2(bertmodel)\n",
    "\n",
    "summary(RNA_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNA_model(next(iter(DataLoader(rna_dataset, 3)))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.0453e+00, -1.0082e-01, -8.5790e-02,  ...,  6.9567e-01,\n",
       "           9.0658e-01,  3.9749e-01],\n",
       "         [-9.6738e-02, -8.4663e-01,  1.2376e-01,  ...,  5.4337e-01,\n",
       "           9.8046e-01,  8.4537e-01]],\n",
       "\n",
       "        [[-6.1251e-04,  6.8470e-01, -2.9411e-01,  ...,  1.5148e+00,\n",
       "          -4.5277e-03,  8.0242e-01],\n",
       "         [ 8.4973e-02, -2.8476e-01, -8.3920e-01,  ...,  3.9015e-02,\n",
       "           4.3316e-01,  3.0112e-01]],\n",
       "\n",
       "        [[-1.0954e+00,  8.2157e-01, -3.0471e-01,  ..., -4.3983e-02,\n",
       "           1.2286e+00,  1.2809e+00],\n",
       "         [-7.6830e-01, -1.9092e-01, -1.8542e+00,  ...,  8.1878e-03,\n",
       "           6.4648e-02,  1.1038e+00]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNA_model(next(iter(DataLoader(rna_dataset, 3)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-100., -100., -100.,  ..., -100., -100., -100.],\n",
       "         [-100., -100., -100.,  ..., -100., -100., -100.]],\n",
       "\n",
       "        [[-100., -100., -100.,  ..., -100., -100., -100.],\n",
       "         [-100., -100., -100.,  ..., -100., -100., -100.]],\n",
       "\n",
       "        [[-100., -100., -100.,  ..., -100., -100., -100.],\n",
       "         [-100., -100., -100.,  ..., -100., -100., -100.]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DataLoader(rna_dataset, 3)))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DataLoader(rna_dataset, 3)))[0].dtype, next(iter(DataLoader(rna_dataset, 3)))[1].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model by masking tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name    | Type               | Params\n",
      "-----------------------------------------------\n",
      "0 | model   | BERTCustomMasked   | 11.1 M\n",
      "1 | loss_fn | CrossEntropyLoss   | 0     \n",
      "2 | acc_fn  | MulticlassAccuracy | 0     \n",
      "-----------------------------------------------\n",
      "11.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 M    Total params\n",
      "44.241    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-03.\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/torchmetrics/functional/classification/accuracy.py:65: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  tp = tp.sum(dim=0 if multidim_average == \"global\" else 1)\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 99/100 [00:40<00:00,  2.47it/s, v_num=7, train_loss=0.0441, train_accuracy=0.341]Adjusting learning rate of group 0 to 9.0451e-04.\n",
      "Epoch 0: 100%|██████████| 100/100 [00:40<00:00,  2.47it/s, v_num=7, train_loss=0.0441, train_accuracy=0.323]\n",
      "Epoch 0, Avg. Training Loss: 0.047 Avg. Training Accuracy: 0.301 Avg. Validation Loss: 0.049 Avg. Validation Accuracy: 0.280\n",
      "Epoch 1:  99%|█████████▉| 99/100 [00:41<00:00,  2.41it/s, v_num=7, train_loss=0.044, train_accuracy=0.333, val_loss=0.0446, val_accuracy=0.313]  Adjusting learning rate of group 0 to 6.5451e-04.\n",
      "Epoch 1: 100%|██████████| 100/100 [00:41<00:00,  2.41it/s, v_num=7, train_loss=0.0428, train_accuracy=0.422, val_loss=0.0446, val_accuracy=0.313]\n",
      "Epoch 1, Avg. Training Loss: 0.045 Avg. Training Accuracy: 0.314 Avg. Validation Loss: 0.045 Avg. Validation Accuracy: 0.325\n",
      "Epoch 2:  99%|█████████▉| 99/100 [00:41<00:00,  2.39it/s, v_num=7, train_loss=0.0448, train_accuracy=0.318, val_loss=0.0447, val_accuracy=0.322] Adjusting learning rate of group 0 to 3.4549e-04.\n",
      "Epoch 2: 100%|██████████| 100/100 [00:41<00:00,  2.39it/s, v_num=7, train_loss=0.0436, train_accuracy=0.302, val_loss=0.0447, val_accuracy=0.322]\n",
      "Epoch 2, Avg. Training Loss: 0.044 Avg. Training Accuracy: 0.309 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.315\n",
      "Epoch 3:  99%|█████████▉| 99/100 [00:40<00:00,  2.43it/s, v_num=7, train_loss=0.0444, train_accuracy=0.346, val_loss=0.0442, val_accuracy=0.318] Adjusting learning rate of group 0 to 9.5492e-05.\n",
      "Epoch 3: 100%|██████████| 100/100 [00:41<00:00,  2.43it/s, v_num=7, train_loss=0.0466, train_accuracy=0.354, val_loss=0.0442, val_accuracy=0.318]\n",
      "Epoch 3, Avg. Training Loss: 0.044 Avg. Training Accuracy: 0.321 Avg. Validation Loss: 0.044 Avg. Validation Accuracy: 0.301\n",
      "Epoch 3: 100%|██████████| 100/100 [00:42<00:00,  2.35it/s, v_num=7, train_loss=0.0466, train_accuracy=0.354, val_loss=0.0444, val_accuracy=0.300]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 13/13 [00:01<00:00,  9.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.323074072599411     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.04438428953289986    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.323074072599411    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04438428953289986   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.04438428953289986, 'test_accuracy': 0.323074072599411}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.dataset import MaskedDataModule\n",
    "from python_scripts.transformers.task import MaskingTask\n",
    "\n",
    "masked_datamodule = MaskedDataModule(masked_dataset, batch_size=8)\n",
    "\n",
    "masked_optimizer = torch.optim.Adam(masked_model.parameters(), 1e-3)\n",
    "masked_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    masked_optimizer,\n",
    "    T_max=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "maskingtask = MaskingTask(\n",
    "    model=masked_model,\n",
    "    loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=masked_optimizer,\n",
    "    scheduler=masked_scheduler,\n",
    "    acc_fn=Accuracy(task='multiclass', num_classes=len(masked_dataset.vocab), ignore_index=-100)\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor='val_avg_accuracy',\n",
    "    save_top_k=2,\n",
    "    mode='max'\n",
    "))\n",
    "callbacks.append(EarlyStopping(\n",
    "    monitor='val_avg_accuracy',\n",
    "    min_delta=0.1,\n",
    "    patience=3,\n",
    "    verbose=False,\n",
    "    mode='max'\n",
    "))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# maskingtask = MaskingTask.load_from_checkpoint(\n",
    "#     './lightning_logs/version_0/checkpoints/epoch=0-step=33562.ckpt',\n",
    "#     model=masked_model,\n",
    "#     loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "#     optimizer=masked_optimizer,\n",
    "#     scheduler=masked_scheduler,\n",
    "#     acc_fn=Accuracy(task='multiclass', num_classes=len(masked_dataset.vocab), ignore_index=-100)\n",
    "# )\n",
    "\n",
    "trainer.fit(maskingtask, datamodule=masked_datamodule)\n",
    "trainer.test(maskingtask, datamodule=masked_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name  | Type                      | Params\n",
      "----------------------------------------------------\n",
      "0 | model | BERTCustomRNAReactivity_2 | 2.8 M \n",
      "----------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.112    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:370: UserWarning: You have overridden `on_before_batch_transfer` in `LightningModule` but have passed in a `LightningDataModule`. It will use the implementation from `LightningModule` instance.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/100 [00:02<04:39,  2.83s/it, v_num=7, train_loss=0.636]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/torch/autograd/__init__.py:200: UserWarning: The operator 'aten::sgn.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 100/100 [00:32<00:00,  3.09it/s, v_num=7, train_loss=0.365]\n",
      "Epoch 0, Avg. Training Loss: 0.4895 Avg. Validation Loss: 0.3142\n",
      "0.0007585010606532027\n",
      "Epoch 1: 100%|██████████| 100/100 [00:30<00:00,  3.28it/s, v_num=7, train_loss=0.270, val_loss=0.276]\n",
      "Epoch 1, Avg. Training Loss: 0.3200 Avg. Validation Loss: 0.2309\n",
      "0.0009485190986744373\n",
      "Epoch 2: 100%|██████████| 100/100 [00:31<00:00,  3.16it/s, v_num=7, train_loss=0.260, val_loss=0.231]\n",
      "Epoch 2, Avg. Training Loss: 0.2758 Avg. Validation Loss: 0.2234\n",
      "0.0006068809706154799\n",
      "Epoch 3: 100%|██████████| 100/100 [00:30<00:00,  3.29it/s, v_num=7, train_loss=0.269, val_loss=0.223]\n",
      "Epoch 3, Avg. Training Loss: 0.2609 Avg. Validation Loss: 0.2188\n",
      "0.00018475966821879009\n",
      "Epoch 4: 100%|██████████| 100/100 [00:30<00:00,  3.29it/s, v_num=7, train_loss=0.267, val_loss=0.219]\n",
      "Epoch 4, Avg. Training Loss: 0.2553 Avg. Validation Loss: 0.2199\n",
      "2.114189442253868e-08\n",
      "Epoch 4: 100%|██████████| 100/100 [00:31<00:00,  3.16it/s, v_num=7, train_loss=0.267, val_loss=0.220]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 100/100 [00:31<00:00,  3.15it/s, v_num=7, train_loss=0.267, val_loss=0.220]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 13/13 [00:01<00:00, 11.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.21816213428974152    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.21816213428974152   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.21816213428974152}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.dataset import RNADataModule\n",
    "from python_scripts.transformers.task import RNATask\n",
    "\n",
    "rna_datamodule = RNADataModule(rna_dataset, batch_size=8)\n",
    "\n",
    "def rna_rmse_loss(x: torch.tensor, y: torch.tensor, ignore_index=-100):\n",
    "    not_ignore = y != ignore_index\n",
    "    return torch.sqrt(torch.square(x[not_ignore] - y[not_ignore]).mean())\n",
    "\n",
    "def rna_mse_loss(x: torch.tensor, y: torch.tensor, ignore_index=-100):\n",
    "    not_ignore = y != ignore_index\n",
    "    return torch.square(x[not_ignore] - y[not_ignore]).mean()\n",
    "\n",
    "def rna_mae_loss(x: torch.tensor, y: torch.tensor, ignore_index=-100):\n",
    "    not_ignore = y != ignore_index\n",
    "    return torch.abs(x[not_ignore] - y[not_ignore]).mean()\n",
    "\n",
    "# rna_optimizer = torch.optim.Adam(RNA_model.parameters(), 1e-3)\n",
    "rna_optimizer = torch.optim.SGD(RNA_model.parameters(), 1e-3, 0.9)\n",
    "# rna_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     rna_optimizer,\n",
    "#     T_max=5,\n",
    "#     eta_min=1e-4,\n",
    "#     verbose=True,\n",
    "# )\n",
    "# rna_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     rna_optimizer,\n",
    "#     [4, 7, 10, 13, 16, 19],\n",
    "#     verbose=True,\n",
    "#     gamma=0.3\n",
    "# )\n",
    "# rna_scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "#     optimizer=rna_optimizer,\n",
    "#     base_lr=1e-6,\n",
    "#     max_lr=1e-3,\n",
    "#     step_size_up=3000,\n",
    "#     step_size_down=7000,\n",
    "#     verbose=True\n",
    "# )\n",
    "rna_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=rna_optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=5,\n",
    "    div_factor=1e2,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "rna_task = RNATask(\n",
    "    model=RNA_model,\n",
    "    loss_fn=rna_mae_loss,\n",
    "    optimizer=rna_optimizer,\n",
    "    scheduler=rna_scheduler,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    monitor='val_avg_loss',\n",
    "    save_top_k=3,\n",
    "    mode='min'\n",
    "))\n",
    "# callbacks.append(EarlyStopping(\n",
    "#     monitor='val_avg_loss',\n",
    "#     min_delta=0.001,\n",
    "#     patience=3,\n",
    "#     verbose=True,\n",
    "#     mode='min'\n",
    "# ))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# trainer = pl.Trainer(resume_from_checkpoint='../notebooks/lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt')\n",
    "\n",
    "trainer.fit(rna_task, datamodule=rna_datamodule)\n",
    "trainer.test(rna_task, datamodule=rna_datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_min</th>\n",
       "      <th>id_max</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>future</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>eee73c1836bc</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAUUUCCUUCCAAAUCCUGAGG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>177</td>\n",
       "      <td>353</td>\n",
       "      <td>d2a929af7a97</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAUGUAAUCAGAUUGCUUCUCC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>354</td>\n",
       "      <td>530</td>\n",
       "      <td>d39a4425ff45</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAACACAUGAAUUUGAGGGUU...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>531</td>\n",
       "      <td>707</td>\n",
       "      <td>1fc41e92d553</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAUCAGAGCUGGCAAAUGGAUG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>708</td>\n",
       "      <td>884</td>\n",
       "      <td>1d0826fb892f</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAUUUGGUAUUUGAUGCAUUAA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_min  id_max   sequence_id  \\\n",
       "0       0     176  eee73c1836bc   \n",
       "1     177     353  d2a929af7a97   \n",
       "2     354     530  d39a4425ff45   \n",
       "3     531     707  1fc41e92d553   \n",
       "4     708     884  1d0826fb892f   \n",
       "\n",
       "                                            sequence  future  \n",
       "0  GGGAACGACUCGAGUAGAGUCGAAAAUUUCCUUCCAAAUCCUGAGG...       0  \n",
       "1  GGGAACGACUCGAGUAGAGUCGAAAAUGUAAUCAGAUUGCUUCUCC...       0  \n",
       "2  GGGAACGACUCGAGUAGAGUCGAAAAAACACAUGAAUUUGAGGGUU...       0  \n",
       "3  GGGAACGACUCGAGUAGAGUCGAAAAUCAGAGCUGGCAAAUGGAUG...       0  \n",
       "4  GGGAACGACUCGAGUAGAGUCGAAAAUUUGGUAUUUGAUGCAUUAA...       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data_pd = pd.read_csv(TEST_DATA_PATH)\n",
    "test_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>sequence_ext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAUUUCCUUCCAAAUCCUGAGG...</td>\n",
       "      <td>(((((((((((.....)))))).....)))))......(((((((....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAUGUAAUCAGAUUGCUUCUCC...</td>\n",
       "      <td>.....((((((.....))))))................((..((((...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAAACACAUGAAUUUGAGGGUU...</td>\n",
       "      <td>.....((((((.....))))))...............((((((.(....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAUCAGAGCUGGCAAAUGGAUG...</td>\n",
       "      <td>.....((((((.....))))))....((.(((..((((((.((.((...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAUUUGGUAUUUGAUGCAUUAA...</td>\n",
       "      <td>.....((((((.....)))))).................((........</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  GGGAACGACUCGAGUAGAGUCGAAAAUUUCCUUCCAAAUCCUGAGG...   \n",
       "1  GGGAACGACUCGAGUAGAGUCGAAAAUGUAAUCAGAUUGCUUCUCC...   \n",
       "2  GGGAACGACUCGAGUAGAGUCGAAAAAACACAUGAAUUUGAGGGUU...   \n",
       "3  GGGAACGACUCGAGUAGAGUCGAAAAUCAGAGCUGGCAAAUGGAUG...   \n",
       "4  GGGAACGACUCGAGUAGAGUCGAAAAUUUGGUAUUUGAUGCAUUAA...   \n",
       "\n",
       "                                        sequence_ext  \n",
       "0  (((((((((((.....)))))).....)))))......(((((((....  \n",
       "1  .....((((((.....))))))................((..((((...  \n",
       "2  .....((((((.....))))))...............((((((.(....  \n",
       "3  .....((((((.....))))))....((.(((..((((((.((.((...  \n",
       "4  .....((((((.....)))))).................((........  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_extracted_pd = pd.read_csv(TEST_DATA_EXT_PATH)\n",
    "test_extracted_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, predict_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/167978 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:370: UserWarning: You have overridden `on_before_batch_transfer` in `LightningModule` but have passed in a `LightningDataModule`. It will use the implementation from `LightningModule` instance.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 3/167978 [00:00<8:35:18,  5.43it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/loops/prediction_loop.py:233: UserWarning: predict returned None if it was on purpose, ignore this warning...\n",
      "  self._warning_cache.warn(\"predict returned None if it was on purpose, ignore this warning...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 328/167978 [00:30<4:17:10, 10.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/autotrading/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts.transformers.dataset import RNADataset_3\n",
    "from python_scripts.transformers.dataset import RNADataModule\n",
    "from python_scripts.transformers.task import RNATask\n",
    "\n",
    "rna_extracted_dataset = RNADataset_3(\n",
    "    data_ext = test_extracted_pd,\n",
    "    vocab=pd.read_csv('../data/RNA folding/vocab.csv'),\n",
    "    max_len=512\n",
    ")\n",
    "\n",
    "rna_datamodule = RNADataModule(rna_dataset, predict_dataset=rna_extracted_dataset, batch_size=8)\n",
    "\n",
    "def rna_mae_loss(x: torch.tensor, y: torch.tensor, ignore_index=-100):\n",
    "    not_ignore = y != ignore_index\n",
    "    return torch.abs(x[not_ignore] - y[not_ignore]).mean()\n",
    "\n",
    "# rna_optimizer = torch.optim.Adam(RNA_model.parameters(), 1e-3)\n",
    "rna_optimizer = torch.optim.SGD(RNA_model.parameters(), 1e-3, 0.9)\n",
    "# rna_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     rna_optimizer,\n",
    "#     T_max=5,\n",
    "#     eta_min=1e-4,\n",
    "#     verbose=True,\n",
    "# )\n",
    "# rna_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     rna_optimizer,\n",
    "#     [4, 7, 10, 13, 16, 19],\n",
    "#     verbose=True,\n",
    "#     gamma=0.3\n",
    "# )\n",
    "# rna_scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "#     optimizer=rna_optimizer,\n",
    "#     base_lr=1e-6,\n",
    "#     max_lr=1e-3,\n",
    "#     step_size_up=3000,\n",
    "#     step_size_down=7000,\n",
    "#     verbose=True\n",
    "# )\n",
    "rna_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=rna_optimizer,\n",
    "    max_lr=1e-3,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=5,\n",
    "    div_factor=1e2,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "rna_task = RNATask.load_from_checkpoint(\n",
    "    checkpoint_path='./lightning_logs/Small_set_5epochs/checkpoints/epoch=4-step=500.ckpt',\n",
    "    model=RNA_model,\n",
    "    loss_fn=rna_mae_loss,\n",
    "    optimizer=rna_optimizer,\n",
    "    scheduler=rna_scheduler,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer()\n",
    "\n",
    "trainer.predict(rna_task, datamodule=rna_datamodule, return_predictions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(SUBMISSION_SAMPLE_FILE_PATH)\n",
    "submission['reactivity_2A3_MaP'] = rna_task.predict_outputs['2A3_MaP']\n",
    "submission['reactivity_DMS_MaP'] = rna_task.predict_outputs['DMS_MaP']\n",
    "submission.to_csv(SUBMISSION_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt')\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
