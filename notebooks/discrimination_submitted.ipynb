{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDT_AI-classifying prejudice and discrimination texts\n",
    "https://www.kaggle.com/competitions/kdtai-2/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts import data_setup, engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {\n",
    "  0: 'Origin(출신차별)',\n",
    "  1: 'Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.',\n",
    "  2: 'Politics(정치성향차별)',\n",
    "  3: 'Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포함합니다.',\n",
    "  4: 'Age(연령차별)',\n",
    "  5: 'Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.',\n",
    "  6: 'Not Hate Speech(해당사항없음)',\n",
    "}\n",
    "\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>유소영비호감 성형아줌마</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>나오지마라 썅</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65858</th>\n",
       "      <td>65858</td>\n",
       "      <td>ㅋ ㅋ 쇼~~~ 도 적당히</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65859</th>\n",
       "      <td>65859</td>\n",
       "      <td>\"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65860</th>\n",
       "      <td>65860</td>\n",
       "      <td>조센징들은 참 피곤하게 산다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65861</th>\n",
       "      <td>65861</td>\n",
       "      <td>\"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65862</th>\n",
       "      <td>65862</td>\n",
       "      <td>항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65863 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                            text  label\n",
       "0          0                                    유소영비호감 성형아줌마      1\n",
       "1          1                                         나오지마라 썅      3\n",
       "2          2            식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!      6\n",
       "3          3                              성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ      5\n",
       "4          4                       \"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"      0\n",
       "...      ...                                             ...    ...\n",
       "65858  65858                                  ㅋ ㅋ 쇼~~~ 도 적당히      6\n",
       "65859  65859  \"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"      0\n",
       "65860  65860                                 조센징들은 참 피곤하게 산다      0\n",
       "65861  65861                      \"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"      2\n",
       "65862  65862               항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..      5\n",
       "\n",
       "[65863 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/Discrimination/train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in train_data['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_korean_text(text):\n",
    "    # Remove URLs and mentions <= 본 문제에서는 url 이나 email 주소가 나오지 않으니 그리 크게 중요하지 않을듯\n",
    "    text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "    # Tokenize text using Mecab <= 이 부분은 다른 라이브러리로 바꾸어도 무방함\n",
    "    mecab = Mecab()\n",
    "    tokens = mecab.morphs(text)\n",
    "\n",
    "    # Remove stop words (optional) <= 필요하다면 바꾸어 보아도 됨\n",
    "    stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Remove punctuation and non-Korean characters <= 필요하다면 바꾸어 보아도 됨\n",
    "    tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '지금', '뭐하고', '있느냐']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_korean_text('나는 지금 뭐하고 있느냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebcaa5e6c2a34981ba94e3b19fe16f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making word maps:   0%|          | 0/65863 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_index_to_key = []\n",
    "word_key_to_index = {}\n",
    "\n",
    "for i in tqdm_notebook(range(len(train_data)), 'Making word maps'):\n",
    "    text = train_data.iloc[i]['text']\n",
    "    tokens = preprocess_korean_text(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token not in word_key_to_index:\n",
    "            word_key_to_index[token] = len(word_index_to_key)\n",
    "            word_index_to_key.append(token)\n",
    "\n",
    "word_key_to_index['<unk>'] = len(word_index_to_key)\n",
    "word_index_to_key.append('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36746"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, data, preprocess_korean_text, max_length=100):\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_korean_text = preprocess_korean_text\n",
    "        self.idx_to_class = sorted(data['label'].unique())\n",
    "        self.class_to_idx = {}\n",
    "        for i in range(len(self.idx_to_class)):\n",
    "            self.class_to_idx[self.idx_to_class[i]] = i\n",
    "        self.class_names = self.idx_to_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, \"text\"]\n",
    "        label = self.data.loc[index, \"label\"]\n",
    "\n",
    "        # Preprocess text using the preprocess_korean_text() function\n",
    "        tokens = self.preprocess_korean_text(text)\n",
    "        # Truncate or pad tokens to a fixed length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens += [\"\"] * (self.max_length - len(tokens))\n",
    "\n",
    "        # Convert tokens to indices using the pre-trained GloVe or Word2Vec embeddings\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in word_key_to_index:\n",
    "                indices.append(word_key_to_index[token])\n",
    "            else:\n",
    "                indices.append(word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "        return torch.tensor(indices), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting dataset of length 65863 into splits of size: 65797 and 66\n"
     ]
    }
   ],
   "source": [
    "train_dataset = KoreanTextDataset(\n",
    "    data=train_data,\n",
    "    preprocess_korean_text=preprocess_korean_text,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "train_dataset_sub, val_dataset_sub = data_setup.split_dataset(\n",
    "    dataset=train_dataset,\n",
    "    split_size=0.999,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "\n",
    "        repeated_hidden = hidden.unsqueeze(0).repeat(max_len, 1, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=0).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, LSTM_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=LSTM_layers, bidirectional=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 200, 100]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        lstm_outputs, (hidden, _) = self.lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [200, 64, 1024]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\n",
    "        # print('outputs, hidden: ', pre_lstm_outputs.shape, hidden.shape)\n",
    "\n",
    "        h = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        # [64, 1024]\n",
    "        # print('h: ', h.shape)\n",
    "\n",
    "        attention_weights = self.attention(h, lstm_outputs)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [200, 64, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(lstm_outputs.permute(1, 2, 0), attention_weights.permute(1, 0, 2)).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(context_vector.squeeze(0)))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [1e-4] # 각 LR 별로 10 epoch 씩 연달아 학습 진행\n",
    "weight_decay_list = [1e-4]\n",
    "epochs_list = [1]\n",
    "batch_size_list = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6], 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names, num_classes = train_dataset.class_names, len(train_dataset.class_names)\n",
    "class_names, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_LSTM_attention(\n",
    "    vocab_size=len(word_index_to_key),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=512,\n",
    "    output_dim=num_classes,\n",
    "    LSTM_layers=2,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "RNN_LSTM_attention                       --\n",
       "├─Embedding: 1-1                         (3,674,600)\n",
       "├─LSTM: 1-2                              8,814,592\n",
       "├─Dropout: 1-3                           --\n",
       "├─Attention: 1-4                         --\n",
       "│    └─Linear: 2-1                       2,098,176\n",
       "│    └─Linear: 2-2                       1,024\n",
       "├─Linear: 1-5                            7,175\n",
       "=================================================================\n",
       "Total params: 14,595,567\n",
       "Trainable params: 10,920,967\n",
       "Non-trainable params: 3,674,600\n",
       "================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f0d3a5cfde4fbfb93540b7ea852837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LSTM_attention_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d3722578fa4fd7beebdf38cb61eea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/1029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tuning_results \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mHP_tune_train(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model_generator\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_weights\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLSTM_attention_discrimination\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset_sub,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     test_dataset\u001b[39m=\u001b[39;49mval_dataset_sub,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     learning_rate_list\u001b[39m=\u001b[39;49mlearning_rate_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     weight_decay_list\u001b[39m=\u001b[39;49mweight_decay_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     epochs_list\u001b[39m=\u001b[39;49mepochs_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     batch_size_list\u001b[39m=\u001b[39;49mbatch_size_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     is_tensorboard_writer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     gradient_accumulation_num\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     saving_max\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     metric_learning\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination_submitted.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:268\u001b[0m, in \u001b[0;36mHP_tune_train\u001b[0;34m(model, model_generator, model_weights, model_name, train_dataset, test_dataset, class_names, learning_rate_list, weight_decay_list, epochs_list, batch_size_list, is_tensorboard_writer, device, gradient_accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m is_tensorboard_writer:\n\u001b[1;32m    262\u001b[0m     writer \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcreate_writer(\n\u001b[1;32m    263\u001b[0m         experiment_name\u001b[39m=\u001b[39m model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_test\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    264\u001b[0m         model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m    265\u001b[0m         extra\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLR_\u001b[39m\u001b[39m{\u001b[39;00mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m_WD_\u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m_EP_\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m_BS_\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m_GA_\u001b[39m\u001b[39m{\u001b[39;00mgradient_accumulation_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n\u001b[0;32m--> 268\u001b[0m model_results \u001b[39m=\u001b[39m train_tensorboard_gradient_accumulation(\n\u001b[1;32m    269\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    270\u001b[0m     save_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m_LR_\u001b[39;49m\u001b[39m{\u001b[39;49;00mlearning_rate\u001b[39m}\u001b[39;49;00m\u001b[39m_WD_\u001b[39;49m\u001b[39m{\u001b[39;49;00mweight_decay\u001b[39m}\u001b[39;49;00m\u001b[39m_BS_\u001b[39;49m\u001b[39m{\u001b[39;49;00mbatch_size\u001b[39m}\u001b[39;49;00m\u001b[39m_GA_\u001b[39;49m\u001b[39m{\u001b[39;49;00mgradient_accumulation_num\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    271\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    272\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m    273\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),\n\u001b[1;32m    274\u001b[0m     optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(\n\u001b[1;32m    275\u001b[0m         params\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mparameters(),\n\u001b[1;32m    276\u001b[0m         lr\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    277\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mweight_decay\n\u001b[1;32m    278\u001b[0m     ),\n\u001b[1;32m    279\u001b[0m     accuracy_fn\u001b[39m=\u001b[39;49mAccuracy(\n\u001b[1;32m    280\u001b[0m         task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmulticlass\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    281\u001b[0m         num_classes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(class_names)\n\u001b[1;32m    282\u001b[0m     ),\n\u001b[1;32m    283\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    284\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    285\u001b[0m     writer\u001b[39m=\u001b[39;49mwriter,\n\u001b[1;32m    286\u001b[0m     accumulation_num\u001b[39m=\u001b[39;49mgradient_accumulation_num,\n\u001b[1;32m    287\u001b[0m     saving_max\u001b[39m=\u001b[39;49msaving_max,\n\u001b[1;32m    288\u001b[0m     metric_learning\u001b[39m=\u001b[39;49mmetric_learning\n\u001b[1;32m    289\u001b[0m )\n\u001b[1;32m    291\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    292\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:154\u001b[0m, in \u001b[0;36mtrain_tensorboard_gradient_accumulation\u001b[0;34m(model, save_name, train_dataloader, test_dataloader, loss_fn, optimizer, accuracy_fn, epochs, device, writer, accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    151\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    153\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(epochs), desc\u001b[39m=\u001b[39msave_name, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 154\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step_gradient_accumulation(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    155\u001b[0m                                        dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    156\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    157\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    158\u001b[0m                                        accuracy_fn\u001b[39m=\u001b[39;49maccuracy_fn,\n\u001b[1;32m    159\u001b[0m                                        device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    160\u001b[0m                                        accumulation_num\u001b[39m=\u001b[39;49maccumulation_num,\n\u001b[1;32m    161\u001b[0m                                        metric_learning\u001b[39m=\u001b[39;49mmetric_learning)\n\u001b[1;32m    162\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    163\u001b[0m                                     dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[1;32m    164\u001b[0m                                     loss_fn\u001b[39m=\u001b[39mloss_fn,\n\u001b[1;32m    165\u001b[0m                                     accuracy_fn\u001b[39m=\u001b[39maccuracy_fn,\n\u001b[1;32m    166\u001b[0m                                     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    167\u001b[0m                                     metric_learning\u001b[39m=\u001b[39mmetric_learning)\n\u001b[1;32m    169\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:51\u001b[0m, in \u001b[0;36mtrain_step_gradient_accumulation\u001b[0;34m(model, dataloader, loss_fn, optimizer, accuracy_fn, device, accumulation_num, metric_learning)\u001b[0m\n\u001b[1;32m     48\u001b[0m acc \u001b[39m=\u001b[39m accuracy_fn(y_pred\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), y_batch_train\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     49\u001b[0m acc_accu \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\n\u001b[0;32m---> 51\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m (iter_total \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m accumulation_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuning_results = engine.HP_tune_train(\n",
    "    model=model,\n",
    "    model_generator=None,\n",
    "    model_weights=None,\n",
    "    model_name='LSTM_attention_discrimination',\n",
    "    train_dataset=train_dataset_sub,\n",
    "    test_dataset=val_dataset_sub,\n",
    "    class_names=class_names,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    weight_decay_list=weight_decay_list,\n",
    "    epochs_list=epochs_list,\n",
    "    batch_size_list=batch_size_list,\n",
    "    is_tensorboard_writer=False,\n",
    "    device=device,\n",
    "    gradient_accumulation_num=1,\n",
    "    saving_max=False,\n",
    "    metric_learning=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../models/discrimination/LSTM_attention_discrimination_full_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_weight = torch.load('..\\models\\discrimination\\LSTM_attention_discrimination_full_5.pth')\n",
    "model.load_state_dict(loaded_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b278b52cfdde4fe3b4967e15f87f816f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../data/Discrimination/test.csv')\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for i in tqdm_notebook(range(len(test_data))):\n",
    "    test_text = test_data.loc[i, \"text\"]\n",
    "    test_tokens = preprocess_korean_text(test_text)\n",
    "    if len(test_tokens) > max_length:\n",
    "        test_tokens = test_tokens[:max_length]\n",
    "    else:\n",
    "        test_tokens += [\"\"] * (max_length - len(test_tokens))\n",
    "\n",
    "    indices = []\n",
    "    for token in test_tokens:\n",
    "      if token in word_key_to_index:\n",
    "        indices.append(word_key_to_index[token])\n",
    "      else:\n",
    "        indices.append(word_key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "    test_logits = model(torch.tensor(indices).unsqueeze(0).to(device))\n",
    "    labels.append(class_names[torch.argmax(test_logits.squeeze(0).cpu())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"솔직히 우리나라 청년들도 불쌍하고 아재들도 불쌍하고 노인들도 불쌍하다. 나라가 참\"</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>그만 보고싶네요 .늙은애들은.</td>\n",
       "      <td>Age(연령차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"더러운 개신교벌레 새퀴\"</td>\n",
       "      <td>Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>근데전태수씨 사망이유가뭔가요그어떤기사에도 나오질않네요</td>\n",
       "      <td>Not Hate Speech(해당사항없음)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"태극기부대와 틀닭바퀴충들에게 순시리는 국모다. ㅉㅉ\"</td>\n",
       "      <td>Politics(정치성향차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13486</th>\n",
       "      <td>13486</td>\n",
       "      <td>\"ㅅㅂ저년 한국인 아니라서 법적으로 어떻게 못하는거아님?OOO아 ㅅㅂ 느그나라로 꺼져\"</td>\n",
       "      <td>Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13487</th>\n",
       "      <td>13487</td>\n",
       "      <td>\"틀딱 택시기사 왔는가\"</td>\n",
       "      <td>Age(연령차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13488</th>\n",
       "      <td>13488</td>\n",
       "      <td>\"틀딱들의 흔한 망상중 하나죠ㅋㅋ\"</td>\n",
       "      <td>Age(연령차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13489</th>\n",
       "      <td>13489</td>\n",
       "      <td>\"빨갱이 새....끼 역시 잔인하노 능지처참은 중국놈인 니놈이고\"</td>\n",
       "      <td>Politics(정치성향차별)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13490</th>\n",
       "      <td>13490</td>\n",
       "      <td>김태리 경희대인데 실업계 고등학교 출신이냐? ㅋㅋㅋ요새 대입전형개판인 틈을 잘이용한...</td>\n",
       "      <td>Origin(출신차별)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13491 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                               text  \\\n",
       "0          0    \"솔직히 우리나라 청년들도 불쌍하고 아재들도 불쌍하고 노인들도 불쌍하다. 나라가 참\"   \n",
       "1          1                                   그만 보고싶네요 .늙은애들은.   \n",
       "2          2                                     \"더러운 개신교벌레 새퀴\"   \n",
       "3          3                      근데전태수씨 사망이유가뭔가요그어떤기사에도 나오질않네요   \n",
       "4          4                     \"태극기부대와 틀닭바퀴충들에게 순시리는 국모다. ㅉㅉ\"   \n",
       "...      ...                                                ...   \n",
       "13486  13486   \"ㅅㅂ저년 한국인 아니라서 법적으로 어떻게 못하는거아님?OOO아 ㅅㅂ 느그나라로 꺼져\"   \n",
       "13487  13487                                      \"틀딱 택시기사 왔는가\"   \n",
       "13488  13488                                \"틀딱들의 흔한 망상중 하나죠ㅋㅋ\"   \n",
       "13489  13489               \"빨갱이 새....끼 역시 잔인하노 능지처참은 중국놈인 니놈이고\"   \n",
       "13490  13490  김태리 경희대인데 실업계 고등학교 출신이냐? ㅋㅋㅋ요새 대입전형개판인 틈을 잘이용한...   \n",
       "\n",
       "                                                   label  \n",
       "0                                Not Hate Speech(해당사항없음)  \n",
       "1                                              Age(연령차별)  \n",
       "2      Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...  \n",
       "3                                Not Hate Speech(해당사항없음)  \n",
       "4                                       Politics(정치성향차별)  \n",
       "...                                                  ...  \n",
       "13486  Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포...  \n",
       "13487                                          Age(연령차별)  \n",
       "13488                                          Age(연령차별)  \n",
       "13489                                   Politics(정치성향차별)  \n",
       "13490                                       Origin(출신차별)  \n",
       "\n",
       "[13491 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['label'] = [idx_to_class[label] for label in labels]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission completed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label\n",
       "0   0      6\n",
       "1   1      4\n",
       "2   2      3\n",
       "3   3      6\n",
       "4   4      2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_data = pd.DataFrame({'ID': range(len(test_data)), 'label': labels})\n",
    "submission_data.to_csv('../submissions/discrimination/submission_LSTM_attention_discrimination_full_5.csv', index=False)\n",
    "print('submission completed!')\n",
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
