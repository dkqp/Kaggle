{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDT_AI-classifying prejudice and discrimination texts\n",
    "https://www.kaggle.com/competitions/kdtai-2/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts import data_setup, engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {\n",
    "  0: 'Origin(출신차별)',\n",
    "  1: 'Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.',\n",
    "  2: 'Politics(정치성향차별)',\n",
    "  3: 'Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포함합니다.',\n",
    "  4: 'Age(연령차별)',\n",
    "  5: 'Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.',\n",
    "  6: 'Not Hate Speech(해당사항없음)',\n",
    "}\n",
    "\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>유소영비호감 성형아줌마</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>나오지마라 썅</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65858</th>\n",
       "      <td>65858</td>\n",
       "      <td>ㅋ ㅋ 쇼~~~ 도 적당히</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65859</th>\n",
       "      <td>65859</td>\n",
       "      <td>\"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65860</th>\n",
       "      <td>65860</td>\n",
       "      <td>조센징들은 참 피곤하게 산다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65861</th>\n",
       "      <td>65861</td>\n",
       "      <td>\"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65862</th>\n",
       "      <td>65862</td>\n",
       "      <td>항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65863 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                            text  label\n",
       "0          0                                    유소영비호감 성형아줌마      1\n",
       "1          1                                         나오지마라 썅      3\n",
       "2          2            식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!      6\n",
       "3          3                              성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ      5\n",
       "4          4                       \"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"      0\n",
       "...      ...                                             ...    ...\n",
       "65858  65858                                  ㅋ ㅋ 쇼~~~ 도 적당히      6\n",
       "65859  65859  \"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"      0\n",
       "65860  65860                                 조센징들은 참 피곤하게 산다      0\n",
       "65861  65861                      \"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"      2\n",
       "65862  65862               항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..      5\n",
       "\n",
       "[65863 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/Discrimination/train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data[train_data['label'].isin([0, 1])].reset_index(drop=True)\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in train_data['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_korean_text(text):\n",
    "    # Remove URLs and mentions\n",
    "    text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "    # Tokenize text using Mecab\n",
    "    mecab = Mecab()\n",
    "    tokens = mecab.morphs(text)\n",
    "\n",
    "    # Remove stop words (optional)\n",
    "    stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Remove punctuation and non-Korean characters\n",
    "    tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '지금', '뭐', '하', '고', '있', '느냐']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_korean_text('나는 지금 뭐하고 있느냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = []\n",
    "# for i in tqdm_notebook(range(len(train_data))):\n",
    "#     tt.append(len(preprocess_korean_text(train_data.iloc[i]['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.boxplot(tt)\n",
    "# print(max(tt), min(tt), np.mean(tt), np.var(tt))\n",
    "# print(np.sum(np.array(tt) > 100), '/', len(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "358043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Word2Vec embedding pre-trained model\n",
    "\n",
    "w2v_pretrained_model = word2vec.Word2Vec.load('../data/Discrimination/word2vec')\n",
    "w2v_pretrained_model.wv.add_vector('<unk>', [0.0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Weight\n",
      "358043 words loaded!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "358043"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting GloVe embedding pre-trained model\n",
    "\n",
    "def load_glove_model(file):\n",
    "    print(\"Loading Glove Weight\")\n",
    "    glove_vector = {}\n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_vector[word] = embedding\n",
    "\n",
    "    class Word_vector():\n",
    "        def __init__(self, key_to_vector) -> None:\n",
    "            self.key_to_vector = key_to_vector\n",
    "\n",
    "            self.index_to_key = []\n",
    "            self.key_to_index = {}\n",
    "            for key in self.key_to_vector.keys():\n",
    "                self.index_to_key.append(key)\n",
    "                self.key_to_index[key] = len(self.index_to_key) - 1\n",
    "\n",
    "            self.vectors = []\n",
    "            for i in range(len(self.index_to_key)):\n",
    "                self.vectors.append(self.key_to_vector[self.index_to_key[i]])\n",
    "            self.vectors = np.array(self.vectors, dtype='float32')\n",
    "\n",
    "            self.vector_size = len(self.vectors[0])\n",
    "\n",
    "        def __contains__(self, key):\n",
    "            return key in self.key_to_vector\n",
    "\n",
    "        def __getitem__(self, key):\n",
    "            return self.key_to_vector[key]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.index_to_key)\n",
    "\n",
    "    class Glove_model():\n",
    "        def __init__(self, vector) -> None:\n",
    "            self.wv = Word_vector(vector)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.wv)\n",
    "\n",
    "    glove_model = Glove_model(glove_vector)\n",
    "\n",
    "    print(f\"{len(glove_model)} words loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "glove_pretrained_model = load_glove_model('../data/Discrimination/glove.txt')\n",
    "len(glove_pretrained_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, data, embed_model, preprocess_korean_text, max_length=100):\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_korean_text = preprocess_korean_text\n",
    "        self.model = embed_model\n",
    "        self.idx_to_class = sorted(data['label'].unique())\n",
    "        self.class_to_idx = {}\n",
    "        for i in range(len(self.idx_to_class)):\n",
    "            self.class_to_idx[self.idx_to_class[i]] = i\n",
    "        self.class_names = self.idx_to_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, \"text\"]\n",
    "        label = self.data.loc[index, \"label\"]\n",
    "\n",
    "        # Preprocess text using the preprocess_korean_text() function\n",
    "        tokens = self.preprocess_korean_text(text)\n",
    "        # Truncate or pad tokens to a fixed length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens += [\"\"] * (self.max_length - len(tokens))\n",
    "\n",
    "        # Convert tokens to indices using the pre-trained GloVe or Word2Vec embeddings\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in self.model.wv:\n",
    "                indices.append(self.model.wv.key_to_index[token])\n",
    "            else:\n",
    "                indices.append(self.model.wv.key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "        return torch.tensor(indices), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting dataset of length 65863 into splits of size: 59276 and 6587\n"
     ]
    }
   ],
   "source": [
    "embed_model = w2v_pretrained_model\n",
    "\n",
    "train_dataset = KoreanTextDataset(\n",
    "    data=train_data,\n",
    "    embed_model=embed_model,\n",
    "    preprocess_korean_text=preprocess_korean_text,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "train_dataset_sub, val_dataset_sub = data_setup.split_dataset(\n",
    "    dataset=train_dataset,\n",
    "    split_size=0.9,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, embedding_model, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        vocab_size = len(embedding_model.wv.index_to_key)\n",
    "        embedding_dim = embedding_model.wv.vector_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, _weight=torch.tensor(embedding_model.wv.vectors))\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "            self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "            self.attention = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 300, 512]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        # packed_input = pack_padded_sequence(embedded, lengths)\n",
    "\n",
    "        outputs, (hidden, _) = self.lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [300, 64, 512]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [2, 64, 512]\n",
    "        # print('outputs, hidden: ', outputs.shape, hidden.shape)\n",
    "\n",
    "        # if self.bidirectional:\n",
    "        #     hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1) #.unsqueeze(1).repeat(1, text.shape[1], 1)\n",
    "        # else:\n",
    "        #     hidden = hidden[-1,:,:] #.unsqueeze(1).repeat(1, text.shape[1], 1)\n",
    "        # hidden = [batch size, hid dim * num directions]: [64, 300, 512]\n",
    "        # print('hidden: ', hidden.shape)\n",
    "\n",
    "        attention_weights = F.softmax(self.attention(outputs.permute(1, 0, 2)), dim=1)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [64, 300, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(outputs.permute(1, 2, 0), attention_weights).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 512]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(context_vector))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [1e-3, 1e-4, 1e-5] # 각 LR 별로 10 epoch 씩 연달아 학습 진행\n",
    "weight_decay_list = [1e-4]\n",
    "epochs_list = [10]\n",
    "batch_size_list = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6], 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names, num_classes = train_dataset.class_names, len(train_dataset.class_names)\n",
    "class_names, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = RNN_LSTM_attention(\n",
    "    embedding_model=w2v_pretrained_model,\n",
    "    hidden_dim=512,\n",
    "    output_dim=num_classes,\n",
    "    num_layers=3,\n",
    "    bidirectional=False,\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = RNN_LSTM_attention(\n",
    "    embedding_model=glove_pretrained_model,\n",
    "    hidden_dim=512,\n",
    "    output_dim=num_classes,\n",
    "    num_layers=3,\n",
    "    bidirectional=False,\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(=================================================================\n",
       " Layer (type:depth-idx)                   Param #\n",
       " =================================================================\n",
       " RNN_LSTM_attention                       --\n",
       " ├─Embedding: 1-1                         (35,804,400)\n",
       " ├─LSTM: 1-2                              5,459,968\n",
       " ├─Dropout: 1-3                           --\n",
       " ├─Linear: 1-4                            3,591\n",
       " ├─Linear: 1-5                            513\n",
       " =================================================================\n",
       " Total params: 41,268,472\n",
       " Trainable params: 5,464,072\n",
       " Non-trainable params: 35,804,400\n",
       " =================================================================,\n",
       " =================================================================\n",
       " Layer (type:depth-idx)                   Param #\n",
       " =================================================================\n",
       " RNN_LSTM_attention                       --\n",
       " ├─Embedding: 1-1                         (35,804,300)\n",
       " ├─LSTM: 1-2                              5,459,968\n",
       " ├─Dropout: 1-3                           --\n",
       " ├─Linear: 1-4                            3,591\n",
       " ├─Linear: 1-5                            513\n",
       " =================================================================\n",
       " Total params: 41,268,372\n",
       " Trainable params: 5,464,072\n",
       " Non-trainable params: 35,804,300\n",
       " =================================================================)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_w2v), summary(model_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b82a6045d541d3bbaec206fad566f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LSTM_attention_w2v_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8303a545d7d84f0d9f5617a536650398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e98c6204c6444e88274e894ae9955e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train_loss: 1.7877, Train_acc: 0.3756 | Test_loss: 1.7461, Test_acc: 0.3874\n",
      "[INFO] Saving model to: ../models/LSTM_attention_w2v_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.38744446635246277.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdca919e80a454083fed1b6b92b3576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8a7f4b29b24b8089b5fe4cd27500fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train_loss: 1.7522, Train_acc: 0.3769 | Test_loss: 1.7312, Test_acc: 0.3874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e0fdac834346ef8235345ae34ee63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tglim/Kaggle/notebooks/discrimination.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m model_w2v\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tuning_results \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mHP_tune_train(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_generator\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model_weights\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLSTM_attention_w2v_discrimination\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset_sub,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test_dataset\u001b[39m=\u001b[39;49mval_dataset_sub,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     learning_rate_list\u001b[39m=\u001b[39;49mlearning_rate_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     weight_decay_list\u001b[39m=\u001b[39;49mweight_decay_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     epochs_list\u001b[39m=\u001b[39;49mepochs_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     batch_size_list\u001b[39m=\u001b[39;49mbatch_size_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     is_tensorboard_writer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     gradient_accumulation_num\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     saving_max\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     metric_learning\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:262\u001b[0m, in \u001b[0;36mHP_tune_train\u001b[0;34m(model, model_generator, model_weights, model_name, train_dataset, test_dataset, class_names, learning_rate_list, weight_decay_list, epochs_list, batch_size_list, is_tensorboard_writer, device, gradient_accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m is_tensorboard_writer:\n\u001b[1;32m    257\u001b[0m     writer \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcreate_writer(\n\u001b[1;32m    258\u001b[0m         experiment_name\u001b[39m=\u001b[39m model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_test\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m    260\u001b[0m         extra\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLR_\u001b[39m\u001b[39m{\u001b[39;00mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m_WD_\u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m_EP_\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m_BS_\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m_GA_\u001b[39m\u001b[39m{\u001b[39;00mgradient_accumulation_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    261\u001b[0m     )\n\u001b[0;32m--> 262\u001b[0m model_results \u001b[39m=\u001b[39m train_tensorboard_gradient_accumulation(\n\u001b[1;32m    263\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    264\u001b[0m     save_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m_LR_\u001b[39;49m\u001b[39m{\u001b[39;49;00mlearning_rate\u001b[39m}\u001b[39;49;00m\u001b[39m_WD_\u001b[39;49m\u001b[39m{\u001b[39;49;00mweight_decay\u001b[39m}\u001b[39;49;00m\u001b[39m_BS_\u001b[39;49m\u001b[39m{\u001b[39;49;00mbatch_size\u001b[39m}\u001b[39;49;00m\u001b[39m_GA_\u001b[39;49m\u001b[39m{\u001b[39;49;00mgradient_accumulation_num\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    265\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    266\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m    267\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),\n\u001b[1;32m    268\u001b[0m     optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(\n\u001b[1;32m    269\u001b[0m         params\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mparameters(),\n\u001b[1;32m    270\u001b[0m         lr\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    271\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mweight_decay\n\u001b[1;32m    272\u001b[0m     ),\n\u001b[1;32m    273\u001b[0m     accuracy_fn\u001b[39m=\u001b[39;49mAccuracy(\n\u001b[1;32m    274\u001b[0m         task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmulticlass\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    275\u001b[0m         num_classes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(class_names)\n\u001b[1;32m    276\u001b[0m     ),\n\u001b[1;32m    277\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    278\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    279\u001b[0m     writer\u001b[39m=\u001b[39;49mwriter,\n\u001b[1;32m    280\u001b[0m     accumulation_num\u001b[39m=\u001b[39;49mgradient_accumulation_num,\n\u001b[1;32m    281\u001b[0m     saving_max\u001b[39m=\u001b[39;49msaving_max,\n\u001b[1;32m    282\u001b[0m     metric_learning\u001b[39m=\u001b[39;49mmetric_learning\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    285\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    286\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:154\u001b[0m, in \u001b[0;36mtrain_tensorboard_gradient_accumulation\u001b[0;34m(model, save_name, train_dataloader, test_dataloader, loss_fn, optimizer, accuracy_fn, epochs, device, writer, accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    151\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    153\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(epochs), desc\u001b[39m=\u001b[39msave_name, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 154\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step_gradient_accumulation(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    155\u001b[0m                                        dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    156\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    157\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    158\u001b[0m                                        accuracy_fn\u001b[39m=\u001b[39;49maccuracy_fn,\n\u001b[1;32m    159\u001b[0m                                        device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    160\u001b[0m                                        accumulation_num\u001b[39m=\u001b[39;49maccumulation_num,\n\u001b[1;32m    161\u001b[0m                                        metric_learning\u001b[39m=\u001b[39;49mmetric_learning)\n\u001b[1;32m    162\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    163\u001b[0m                                     dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[1;32m    164\u001b[0m                                     loss_fn\u001b[39m=\u001b[39mloss_fn,\n\u001b[1;32m    165\u001b[0m                                     accuracy_fn\u001b[39m=\u001b[39maccuracy_fn,\n\u001b[1;32m    166\u001b[0m                                     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    167\u001b[0m                                     metric_learning\u001b[39m=\u001b[39mmetric_learning)\n\u001b[1;32m    169\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:38\u001b[0m, in \u001b[0;36mtrain_step_gradient_accumulation\u001b[0;34m(model, dataloader, loss_fn, optimizer, accuracy_fn, device, accumulation_num, metric_learning)\u001b[0m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m X_batch_train, y_batch_train \u001b[39min\u001b[39;00m tqdm_notebook(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 38\u001b[0m   X_batch_train, y_batch_train \u001b[39m=\u001b[39m X_batch_train\u001b[39m.\u001b[39;49mto(device), y_batch_train\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m   \u001b[39mif\u001b[39;00m metric_learning:\n\u001b[1;32m     41\u001b[0m     y_logits \u001b[39m=\u001b[39m model(X_batch_train, y_batch_train)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model_w2v\n",
    "\n",
    "tuning_results = engine.HP_tune_train(\n",
    "    model=model,\n",
    "    model_generator=None,\n",
    "    model_weights=None,\n",
    "    model_name='LSTM_attention_w2v_discrimination',\n",
    "    train_dataset=train_dataset_sub,\n",
    "    test_dataset=val_dataset_sub,\n",
    "    class_names=class_names,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    weight_decay_list=weight_decay_list,\n",
    "    epochs_list=epochs_list,\n",
    "    batch_size_list=batch_size_list,\n",
    "    is_tensorboard_writer=False,\n",
    "    device=device,\n",
    "    gradient_accumulation_num=1,\n",
    "    saving_max=True,\n",
    "    metric_learning=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e429a3bf73af44d8a892b677e0587826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 0 13.616570327552987 112.99749035680739\n",
      "55 / 15570\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_data = pd.read_csv('../data/Discrimination/test.csv')\n",
    "\n",
    "# tt = []\n",
    "# for i in tqdm_notebook(range(len(train_data))):\n",
    "#     tt.append(len(preprocess_korean_text(train_data.iloc[i]['text'])))\n",
    "\n",
    "# plt.boxplot(tt)\n",
    "# print(max(tt), min(tt), np.mean(tt), np.var(tt))\n",
    "# print(np.sum(np.array(tt) > 100), '/', len(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0a7815e8cd46d38911831cabbbeac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../data/Discrimination/test.csv')\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for i in tqdm_notebook(range(len(test_data))):\n",
    "    test_text = test_data.loc[i, \"text\"]\n",
    "    test_tokens = preprocess_korean_text(test_text)\n",
    "    if len(test_tokens) > max_length:\n",
    "        test_tokens = test_tokens[:max_length]\n",
    "    else:\n",
    "        test_tokens += [\"\"] * (max_length - len(test_tokens))\n",
    "\n",
    "    indices = []\n",
    "    for token in test_tokens:\n",
    "      if token in embed_model.wv:\n",
    "        indices.append(embed_model.wv.key_to_index[token])\n",
    "      else:\n",
    "        indices.append(embed_model.wv.key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "    test_logits = model(torch.tensor(indices).unsqueeze(0).to(device))\n",
    "    labels.append(class_names[torch.argmax(test_logits.squeeze(0).cpu())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission completed!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label\n",
       "0   0      6\n",
       "1   1      6\n",
       "2   2      6\n",
       "3   3      6\n",
       "4   4      6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_data = pd.DataFrame({'ID': range(len(test_data)), 'label': labels})\n",
    "submission_data.to_csv('../submissions/discrimination/submission.csv')\n",
    "print('submission completed!')\n",
    "submission_data.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
