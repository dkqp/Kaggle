{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KDT_AI-classifying prejudice and discrimination texts\n",
    "https://www.kaggle.com/competitions/kdtai-2/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: <8E58E83E-9235-3324-9B6B-260614F85F69> /Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <3F9923D2-81A5-3EC8-9739-EC0C1C816132> /Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from python_scripts import data_setup, engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {\n",
    "  0: 'Origin(출신차별)',\n",
    "  1: 'Physical(외모차별) 외모(신체, 얼굴) 및 장애인 차별 발언을 포함합니다.',\n",
    "  2: 'Politics(정치성향차별)',\n",
    "  3: 'Profanity(혐오욕설) 욕설,저주,혐오 단어, 비속어 및 기타 혐오 발언을 포함합니다.',\n",
    "  4: 'Age(연령차별)',\n",
    "  5: 'Gender(성차별) 성별 또는 성적 취향에 대한 차별 발언을 포함합니다.',\n",
    "  6: 'Not Hate Speech(해당사항없음)',\n",
    "}\n",
    "\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>유소영비호감 성형아줌마</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>나오지마라 썅</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65858</th>\n",
       "      <td>65858</td>\n",
       "      <td>ㅋ ㅋ 쇼~~~ 도 적당히</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65859</th>\n",
       "      <td>65859</td>\n",
       "      <td>\"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65860</th>\n",
       "      <td>65860</td>\n",
       "      <td>조센징들은 참 피곤하게 산다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65861</th>\n",
       "      <td>65861</td>\n",
       "      <td>\"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65862</th>\n",
       "      <td>65862</td>\n",
       "      <td>항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65863 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID                                            text  label\n",
       "0          0                                    유소영비호감 성형아줌마      1\n",
       "1          1                                         나오지마라 썅      3\n",
       "2          2            식상하고 지긋지긋했는데 잘 끝나네 오예 소리벗고 빤스질러~~!!!      6\n",
       "3          3                              성희롱 당할 얼굴이 아닌데?ㅋㅋㅋ      5\n",
       "4          4                       \"끝까지 해보자~쪽파리 원숭이 자한 쓰레기당\"      0\n",
       "...      ...                                             ...    ...\n",
       "65858  65858                                  ㅋ ㅋ 쇼~~~ 도 적당히      6\n",
       "65859  65859  \"이젠 전라도 종것들 음식 불매다. 태양광 중금속 환경오염물로 만든 식품 사절이다\"      0\n",
       "65860  65860                                 조센징들은 참 피곤하게 산다      0\n",
       "65861  65861                      \"문빠 다모아서 빨갱이한테보내고 행복하게살라고\"      2\n",
       "65862  65862               항마력 딸려서 못보겟음 보는사람들 대부분 환상쩌는여자일듯..      5\n",
       "\n",
       "[65863 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/Discrimination/train.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data[train_data['label'].isin([0, 1])].reset_index(drop=True)\n",
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in train_data['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_korean_text(text):\n",
    "    # Remove URLs and mentions\n",
    "    text = re.sub(r\"(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@(\\w+)\", \"\", text)\n",
    "\n",
    "    # Tokenize text using Mecab\n",
    "    mecab = Mecab()\n",
    "    tokens = mecab.morphs(text)\n",
    "\n",
    "    # Remove stop words (optional)\n",
    "    stop_words = [\"은\", \"는\", \"이\", \"가\", \"을\", \"를\", \"에\", \"의\", \"로\", \"으로\", \"에서\"]\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    # Remove punctuation and non-Korean characters\n",
    "    tokens = [re.sub(r\"[^\\u3131-\\u3163\\uac00-\\ud7a3]+\", \"\", t) for t in tokens]\n",
    "    tokens = [t for t in tokens if t]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나', '지금', '뭐', '하', '고', '있', '느냐']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_korean_text('나는 지금 뭐하고 있느냐?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = []\n",
    "# for i in tqdm_notebook(range(len(train_data))):\n",
    "#     tt.append(len(preprocess_korean_text(train_data.iloc[i]['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.boxplot(tt)\n",
    "# print(max(tt), min(tt), np.mean(tt), np.var(tt))\n",
    "# print(np.sum(np.array(tt) > 100), '/', len(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tglim/miniforge3/envs/tensorflow/lib/python3.8/site-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "358043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting Word2Vec embedding pre-trained model\n",
    "\n",
    "w2v_pretrained_model = word2vec.Word2Vec.load('../data/Discrimination/word2vec')\n",
    "w2v_pretrained_model.wv.add_vector('<unk>', [0.0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Weight\n",
      "358085 words loaded!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "358085"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting GloVe embedding pre-trained model\n",
    "\n",
    "def load_glove_model(file):\n",
    "    print(\"Loading Glove Weight\")\n",
    "    glove_vector = {}\n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "            glove_vector[word] = embedding\n",
    "\n",
    "    # 없는 단어 추가\n",
    "    not_exists = pd.read_csv('../data/Discrimination/not_exist_list.csv')\n",
    "    for i in range(len(not_exists)):\n",
    "        glove_vector[not_exists.loc[i, 'word']] = glove_vector[not_exists.loc[i, 'substitute']]\n",
    "\n",
    "    class Word_vector():\n",
    "        def __init__(self, key_to_vector) -> None:\n",
    "            self.key_to_vector = key_to_vector\n",
    "\n",
    "            self.index_to_key = []\n",
    "            self.key_to_index = {}\n",
    "            for key in self.key_to_vector.keys():\n",
    "                self.index_to_key.append(key)\n",
    "                self.key_to_index[key] = len(self.index_to_key) - 1\n",
    "\n",
    "            self.vectors = []\n",
    "            for i in range(len(self.index_to_key)):\n",
    "                self.vectors.append(self.key_to_vector[self.index_to_key[i]])\n",
    "            self.vectors = np.array(self.vectors, dtype='float32')\n",
    "\n",
    "            self.vector_size = len(self.vectors[0])\n",
    "\n",
    "        def __contains__(self, key):\n",
    "            return key in self.key_to_vector\n",
    "\n",
    "        def __getitem__(self, key):\n",
    "            return self.key_to_vector[key]\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.index_to_key)\n",
    "\n",
    "    class Glove_model():\n",
    "        def __init__(self, vector) -> None:\n",
    "            self.wv = Word_vector(vector)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.wv)\n",
    "\n",
    "    glove_model = Glove_model(glove_vector)\n",
    "\n",
    "    print(f\"{len(glove_model)} words loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "glove_pretrained_model = load_glove_model('../data/Discrimination/glove.txt')\n",
    "len(glove_pretrained_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained 임베딩 벡터에 등록되지 않은 단어 중 주요 단어를 골라 수작업 처리\n",
    "# all = 0\n",
    "# not_exists = {}\n",
    "# not_exists_labels = {}\n",
    "\n",
    "# for i in tqdm_notebook(range(len(train_data))):\n",
    "#     sentence, label = train_data.loc[i, 'text'], train_data.loc[i, 'label']\n",
    "#     for word in preprocess_korean_text(sentence):\n",
    "#         if word not in glove_pretrained_model.wv:\n",
    "#             if word in not_exists:\n",
    "#                 not_exists[word] += 1\n",
    "#             else:\n",
    "#                 not_exists[word] = 1\n",
    "\n",
    "#             if word not in not_exists_labels:\n",
    "#                 not_exists_labels[word] = {n: 0 for n in range(7)}\n",
    "#             not_exists_labels[word][label] += 1\n",
    "#         all += 1\n",
    "\n",
    "# not_exists = sorted(not_exists.items(), key=lambda x: x[1], reverse=True)\n",
    "# not_labels = []\n",
    "# for word, n in not_exists:\n",
    "#     not_labels.append(sorted(not_exists_labels[word].items(), key=lambda x: x[1], reverse=True))\n",
    "# print(all, len(not_exists))\n",
    "# print(not_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_exist_list = pd.DataFrame({'word': not_exists, 'label': not_labels})\n",
    "# not_exist_list.to_csv('../data/Discrimination/not_exist_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, data, embed_model, preprocess_korean_text, max_length=100):\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_korean_text = preprocess_korean_text\n",
    "        self.model = embed_model\n",
    "        self.idx_to_class = sorted(data['label'].unique())\n",
    "        self.class_to_idx = {}\n",
    "        for i in range(len(self.idx_to_class)):\n",
    "            self.class_to_idx[self.idx_to_class[i]] = i\n",
    "        self.class_names = self.idx_to_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, \"text\"]\n",
    "        label = self.data.loc[index, \"label\"]\n",
    "\n",
    "        # Preprocess text using the preprocess_korean_text() function\n",
    "        tokens = self.preprocess_korean_text(text)\n",
    "        # Truncate or pad tokens to a fixed length\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        else:\n",
    "            tokens += [\"\"] * (self.max_length - len(tokens))\n",
    "\n",
    "        # Convert tokens to indices using the pre-trained GloVe or Word2Vec embeddings\n",
    "        indices = []\n",
    "        for token in tokens:\n",
    "            if token in self.model.wv:\n",
    "                indices.append(self.model.wv.key_to_index[token])\n",
    "            else:\n",
    "                indices.append(self.model.wv.key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "        return torch.tensor(indices), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Splitting dataset of length 65863 into splits of size: 59276 and 6587\n"
     ]
    }
   ],
   "source": [
    "embed_model = glove_pretrained_model\n",
    "\n",
    "train_dataset = KoreanTextDataset(\n",
    "    data=train_data,\n",
    "    embed_model=embed_model,\n",
    "    preprocess_korean_text=preprocess_korean_text,\n",
    "    max_length=max_length\n",
    ")\n",
    "\n",
    "train_dataset_sub, val_dataset_sub = data_setup.split_dataset(\n",
    "    dataset=train_dataset,\n",
    "    split_size=0.9,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "\n",
    "        repeated_hidden = hidden.unsqueeze(0).repeat(max_len, 1, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention, dim=0).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, embedding_model, hidden_dim, output_dim, pre_LSTM_layers, post_LSTM_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        vocab_size = len(embedding_model.wv.index_to_key)\n",
    "        embedding_dim = embedding_model.wv.vector_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, _weight=torch.tensor(embedding_model.wv.vectors))\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pre_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=pre_LSTM_layers, bidirectional=False, dropout=dropout)\n",
    "        # self.post_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, num_layers=post_LSTM_layers, bidirectional=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = self.dropout(embedded)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 200, 100]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        pre_lstm_outputs, (hidden, cell) = self.pre_lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [200, 64, 1024]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\n",
    "        # print('outputs, hidden: ', pre_lstm_outputs.shape, hidden.shape)\n",
    "\n",
    "        # h = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        # [64, 1024]\n",
    "        # print('h: ', h.shape)\n",
    "\n",
    "        attention_weights = F.softmax(self.attention(pre_lstm_outputs.permute(1, 0, 2)), dim=1)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [200, 64, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(pre_lstm_outputs.permute(1, 2, 0), attention_weights).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        # _, (hidden, _) = self.post_lstm(context_vector.unsqueeze(0), (hidden, cell))\n",
    "        # hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('hidden: ', hidden.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(context_vector))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM_attention(nn.Module):\n",
    "    def __init__(self, embedding_model, hidden_dim, output_dim, pre_LSTM_layers, post_LSTM_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        vocab_size = len(embedding_model.wv.index_to_key)\n",
    "        embedding_dim = embedding_model.wv.vector_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, _weight=torch.tensor(embedding_model.wv.vectors))\n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.pre_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=pre_LSTM_layers, bidirectional=True, dropout=dropout)\n",
    "        self.post_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, num_layers=post_LSTM_layers, bidirectional=True, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, seq len]\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded = [batch size, seq len, emb dim]: [64, 200, 100]\n",
    "        # print('embedded: ', embedded.shape)\n",
    "\n",
    "        pre_lstm_outputs, (hidden, cell) = self.pre_lstm(embedded.permute(1, 0, 2))\n",
    "        # output = [batch size, seq len, hid dim * num directions]: [200, 64, 1024]\n",
    "        # hidden/cell = [num layers * num directions, batch size, hid dim]: [6, 64, 512]\n",
    "        # print('outputs, hidden: ', pre_lstm_outputs.shape, hidden.shape)\n",
    "\n",
    "        h = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        # [64, 1024]\n",
    "        # print('h: ', h.shape)\n",
    "\n",
    "        attention_weights = self.attention(h, pre_lstm_outputs)\n",
    "        # # attention_weights = [batch size, seq len, 1]: [200, 64, 1]\n",
    "        # print('attention_weights: ', attention_weights.shape)\n",
    "\n",
    "        context_vector = torch.bmm(pre_lstm_outputs.permute(1, 2, 0), attention_weights.permute(1, 0, 2)).squeeze(2)\n",
    "        # # context_vector = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('context_vector: ', context_vector.shape)\n",
    "\n",
    "        _, (hidden, _) = self.post_lstm(context_vector.unsqueeze(0), (hidden, cell))\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden = [batch size, hid dim * num directions]: [64, 1024]\n",
    "        # print('hidden: ', hidden.shape)\n",
    "\n",
    "        out = self.fc(self.dropout(hidden.squeeze(0)))\n",
    "        # out = [batch size, output dim]: [64, 7]\n",
    "        # print('out: ', out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_list = [1e-3, 1e-4] # 각 LR 별로 10 epoch 씩 연달아 학습 진행\n",
    "weight_decay_list = [1e-4]\n",
    "epochs_list = [10]\n",
    "batch_size_list = [64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6], 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names, num_classes = train_dataset.class_names, len(train_dataset.class_names)\n",
    "class_names, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = RNN_LSTM_attention(\n",
    "    embedding_model=w2v_pretrained_model,\n",
    "    hidden_dim=512,\n",
    "    output_dim=num_classes,\n",
    "    pre_LSTM_layers=3,\n",
    "    post_LSTM_layers=3,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = RNN_LSTM_attention(\n",
    "    embedding_model=glove_pretrained_model,\n",
    "    hidden_dim=512,\n",
    "    output_dim=num_classes,\n",
    "    pre_LSTM_layers=3,\n",
    "    post_LSTM_layers=3,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(=================================================================\n",
       " Layer (type:depth-idx)                   Param #\n",
       " =================================================================\n",
       " RNN_LSTM_attention                       --\n",
       " ├─Embedding: 1-1                         (35,804,400)\n",
       " ├─LSTM: 1-2                              5,459,968\n",
       " ├─Dropout: 1-3                           --\n",
       " ├─Linear: 1-4                            513\n",
       " ├─Linear: 1-5                            3,591\n",
       " =================================================================\n",
       " Total params: 41,268,472\n",
       " Trainable params: 5,464,072\n",
       " Non-trainable params: 35,804,400\n",
       " =================================================================,\n",
       " =================================================================\n",
       " Layer (type:depth-idx)                   Param #\n",
       " =================================================================\n",
       " RNN_LSTM_attention                       --\n",
       " ├─Embedding: 1-1                         (35,808,500)\n",
       " ├─LSTM: 1-2                              5,459,968\n",
       " ├─Dropout: 1-3                           --\n",
       " ├─Linear: 1-4                            513\n",
       " ├─Linear: 1-5                            3,591\n",
       " =================================================================\n",
       " Total params: 41,272,572\n",
       " Trainable params: 5,464,072\n",
       " Non-trainable params: 35,808,500\n",
       " =================================================================)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_w2v), summary(model_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7ce696ca544688b1e5d3f09948d0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1:   0%|          | 0/10 [00:00<?, ?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faec3a73b0894d5fa01fcac68bb07379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945b334ec0584430aa41d93063f5ee82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train_loss: 1.1390, Train_acc: 0.5927 | Test_loss: 0.9463, Test_acc: 0.6728\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.6728.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a959e1d01d546fd811390c77ff871f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9417fa8fa384b92b17416104c612415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train_loss: 0.9144, Train_acc: 0.6787 | Test_loss: 0.8945, Test_acc: 0.6867\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_1_TEST-ACC_0.6867.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6b263a29be4db297bb7368eebe852e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6424f8d3e75043e788fb3cbcfa8bda7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train_loss: 0.8666, Train_acc: 0.6939 | Test_loss: 0.8571, Test_acc: 0.6927\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_2_TEST-ACC_0.6927.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd16b69b0254c5b97a3c7bf9572490b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c13e19cb71246da832525fea9916ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train_loss: 0.8363, Train_acc: 0.7061 | Test_loss: 0.8420, Test_acc: 0.7028\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_3_TEST-ACC_0.7028.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b81b59aeca44638bf8c1ef39dbeac09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bba2623f9842c3bf93aa71b36fddf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train_loss: 0.8193, Train_acc: 0.7116 | Test_loss: 0.8221, Test_acc: 0.7145\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_4_TEST-ACC_0.7145.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b16426a219641c4a8e242d4221fb306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a204316cb748df9fd5448403dd3e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train_loss: 0.8034, Train_acc: 0.7183 | Test_loss: 0.8110, Test_acc: 0.7113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3605c2dbb864d058841e53620fd7993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4ac3aed1844822acc493be08d7adce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train_loss: 0.7931, Train_acc: 0.7197 | Test_loss: 0.8068, Test_acc: 0.7183\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_6_TEST-ACC_0.7183.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41a42ae1b0740129ba854cbf440ce0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de170555d3664ca3bf4b2d382daee27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train_loss: 0.7836, Train_acc: 0.7243 | Test_loss: 0.8142, Test_acc: 0.7119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505b43066e89420f85a22f28b5610d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102bf1cf76cc41faa5f1f9ef9ebfdacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train_loss: 0.7770, Train_acc: 0.7238 | Test_loss: 0.7997, Test_acc: 0.7206\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_8_TEST-ACC_0.7206.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c962ea592688486881e3931e7a6dc42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982d29ca0f0746b195363eaf1a17d518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train_loss: 0.7718, Train_acc: 0.7264 | Test_loss: 0.8094, Test_acc: 0.7142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8874ca979c45acb3dcf117ccd45da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LSTM_attention_glove_add_words_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1:   0%|          | 0/10 [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09be2a04101446feb1dc8b566ac73897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd37177436ba4427bf1ec83463be458a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train_loss: 0.7315, Train_acc: 0.7409 | Test_loss: 0.7730, Test_acc: 0.7266\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.7266.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d52c13fe30a4d37bade62769fc47c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d50aa69e00e43d6843e0562e7172635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train_loss: 0.7181, Train_acc: 0.7434 | Test_loss: 0.7663, Test_acc: 0.7315\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1_EPOCH_1_TEST-ACC_0.7315.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2baf29cf8184308bff47f862d34ac06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d71865b2ba74f45a509195b91931092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train_loss: 0.7135, Train_acc: 0.7478 | Test_loss: 0.7664, Test_acc: 0.7304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39184ec2f4e14309a1b2f82e6993f48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbdfae8fa164c939c70b032403bc759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train_loss: 0.7100, Train_acc: 0.7479 | Test_loss: 0.7668, Test_acc: 0.7310\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f488d4d9314a9f9f69344a8ccbee87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e84cb33160942819c9b8722e8366673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train_loss: 0.7079, Train_acc: 0.7493 | Test_loss: 0.7690, Test_acc: 0.7307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679558f465a24535ba1c15e61c65f74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tglim/Kaggle/notebooks/discrimination.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m model_glove\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tuning_results \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mHP_tune_train(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_generator\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model_weights\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLSTM_attention_glove_add_words_discrimination\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset_sub,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test_dataset\u001b[39m=\u001b[39;49mval_dataset_sub,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     learning_rate_list\u001b[39m=\u001b[39;49mlearning_rate_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     weight_decay_list\u001b[39m=\u001b[39;49mweight_decay_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     epochs_list\u001b[39m=\u001b[39;49mepochs_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     batch_size_list\u001b[39m=\u001b[39;49mbatch_size_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     is_tensorboard_writer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     gradient_accumulation_num\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     saving_max\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     metric_learning\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:263\u001b[0m, in \u001b[0;36mHP_tune_train\u001b[0;34m(model, model_generator, model_weights, model_name, train_dataset, test_dataset, class_names, learning_rate_list, weight_decay_list, epochs_list, batch_size_list, is_tensorboard_writer, device, gradient_accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m is_tensorboard_writer:\n\u001b[1;32m    257\u001b[0m     writer \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcreate_writer(\n\u001b[1;32m    258\u001b[0m         experiment_name\u001b[39m=\u001b[39m model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_test\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m    260\u001b[0m         extra\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLR_\u001b[39m\u001b[39m{\u001b[39;00mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m_WD_\u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m_EP_\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m_BS_\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m_GA_\u001b[39m\u001b[39m{\u001b[39;00mgradient_accumulation_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    261\u001b[0m     )\n\u001b[0;32m--> 263\u001b[0m model_results \u001b[39m=\u001b[39m train_tensorboard_gradient_accumulation(\n\u001b[1;32m    264\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    265\u001b[0m     save_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m_LR_\u001b[39;49m\u001b[39m{\u001b[39;49;00mlearning_rate\u001b[39m}\u001b[39;49;00m\u001b[39m_WD_\u001b[39;49m\u001b[39m{\u001b[39;49;00mweight_decay\u001b[39m}\u001b[39;49;00m\u001b[39m_BS_\u001b[39;49m\u001b[39m{\u001b[39;49;00mbatch_size\u001b[39m}\u001b[39;49;00m\u001b[39m_GA_\u001b[39;49m\u001b[39m{\u001b[39;49;00mgradient_accumulation_num\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    266\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    267\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m    268\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),\n\u001b[1;32m    269\u001b[0m     optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(\n\u001b[1;32m    270\u001b[0m         params\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mparameters(),\n\u001b[1;32m    271\u001b[0m         lr\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    272\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mweight_decay\n\u001b[1;32m    273\u001b[0m     ),\n\u001b[1;32m    274\u001b[0m     accuracy_fn\u001b[39m=\u001b[39;49mAccuracy(\n\u001b[1;32m    275\u001b[0m         task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmulticlass\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    276\u001b[0m         num_classes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(class_names)\n\u001b[1;32m    277\u001b[0m     ),\n\u001b[1;32m    278\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    279\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    280\u001b[0m     writer\u001b[39m=\u001b[39;49mwriter,\n\u001b[1;32m    281\u001b[0m     accumulation_num\u001b[39m=\u001b[39;49mgradient_accumulation_num,\n\u001b[1;32m    282\u001b[0m     saving_max\u001b[39m=\u001b[39;49msaving_max,\n\u001b[1;32m    283\u001b[0m     metric_learning\u001b[39m=\u001b[39;49mmetric_learning\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    286\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    287\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:154\u001b[0m, in \u001b[0;36mtrain_tensorboard_gradient_accumulation\u001b[0;34m(model, save_name, train_dataloader, test_dataloader, loss_fn, optimizer, accuracy_fn, epochs, device, writer, accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    151\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    153\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(epochs), desc\u001b[39m=\u001b[39msave_name, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 154\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step_gradient_accumulation(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    155\u001b[0m                                        dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    156\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    157\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    158\u001b[0m                                        accuracy_fn\u001b[39m=\u001b[39;49maccuracy_fn,\n\u001b[1;32m    159\u001b[0m                                        device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    160\u001b[0m                                        accumulation_num\u001b[39m=\u001b[39;49maccumulation_num,\n\u001b[1;32m    161\u001b[0m                                        metric_learning\u001b[39m=\u001b[39;49mmetric_learning)\n\u001b[1;32m    162\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    163\u001b[0m                                     dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[1;32m    164\u001b[0m                                     loss_fn\u001b[39m=\u001b[39mloss_fn,\n\u001b[1;32m    165\u001b[0m                                     accuracy_fn\u001b[39m=\u001b[39maccuracy_fn,\n\u001b[1;32m    166\u001b[0m                                     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    167\u001b[0m                                     metric_learning\u001b[39m=\u001b[39mmetric_learning)\n\u001b[1;32m    169\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:51\u001b[0m, in \u001b[0;36mtrain_step_gradient_accumulation\u001b[0;34m(model, dataloader, loss_fn, optimizer, accuracy_fn, device, accumulation_num, metric_learning)\u001b[0m\n\u001b[1;32m     48\u001b[0m acc \u001b[39m=\u001b[39m accuracy_fn(y_pred\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), y_batch_train\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     49\u001b[0m acc_accu \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\n\u001b[0;32m---> 51\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m (iter_total \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m accumulation_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model_glove\n",
    "\n",
    "tuning_results = engine.HP_tune_train(\n",
    "    model=model,\n",
    "    model_generator=None,\n",
    "    model_weights=None,\n",
    "    model_name='LSTM_attention_glove_add_words_discrimination',\n",
    "    train_dataset=train_dataset_sub,\n",
    "    test_dataset=val_dataset_sub,\n",
    "    class_names=class_names,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    weight_decay_list=weight_decay_list,\n",
    "    epochs_list=epochs_list,\n",
    "    batch_size_list=batch_size_list,\n",
    "    is_tensorboard_writer=False,\n",
    "    device=device,\n",
    "    gradient_accumulation_num=1,\n",
    "    saving_max=True,\n",
    "    metric_learning=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871349676e3a4696b3d37b7b9edca6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1:   0%|          | 0/10 [00:00<?, ?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d168768c1d5b440fbb578976a72f2f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7522fddf98b941a49058aa74d06c1c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train_loss: 1.1242, Train_acc: 0.5993 | Test_loss: 0.9562, Test_acc: 0.6628\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.6628.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6920459e084769b89d2f5bf8021e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3ffbce14a94cadb3db692f4a000374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train_loss: 0.9100, Train_acc: 0.6805 | Test_loss: 0.8698, Test_acc: 0.6939\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_1_TEST-ACC_0.6939.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d06f852e4c4f2facc01ae3a8d58a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6db4c8dfa344c2938863b7390eda85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train_loss: 0.8579, Train_acc: 0.7005 | Test_loss: 0.8455, Test_acc: 0.6996\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_2_TEST-ACC_0.6996.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0eb4eda9c9a4f99b837d50da998a2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c63cc50d1445f9babd6c77129cc580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train_loss: 0.8291, Train_acc: 0.7094 | Test_loss: 0.8280, Test_acc: 0.7100\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_3_TEST-ACC_0.7100.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a3ad76f5034b26bd78726cb5cdbf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2431b5ee7c1040b181e14f8c7dde7da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train_loss: 0.8057, Train_acc: 0.7181 | Test_loss: 0.8142, Test_acc: 0.7103\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_4_TEST-ACC_0.7103.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5874aff20c7c4143b894cde2a506d9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0ad15b6f7c413a8f205da1f923eddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Train_loss: 0.7947, Train_acc: 0.7204 | Test_loss: 0.7981, Test_acc: 0.7153\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_5_TEST-ACC_0.7153.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ea2d1c96704aea9cd7ba180712664a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdf68cc66564de69c685e21de4e0b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Train_loss: 0.7855, Train_acc: 0.7251 | Test_loss: 0.7955, Test_acc: 0.7239\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_6_TEST-ACC_0.7239.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6213d2282044ae9e959596b39da029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3200106dbb1d46de8694baec9a50aeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Train_loss: 0.7761, Train_acc: 0.7252 | Test_loss: 0.7926, Test_acc: 0.7207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e066d8962784fdfb60637200335ce79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32109ee561834088b1573b704be02c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Train_loss: 0.7693, Train_acc: 0.7288 | Test_loss: 0.7814, Test_acc: 0.7232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e83248bcf5c46d39db54d66f3cd9976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40af215a46214d11ae77cb4454288463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Train_loss: 0.7673, Train_acc: 0.7295 | Test_loss: 0.7770, Test_acc: 0.7259\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.001_WD_0.0001_BS_64_GA_1_EPOCH_9_TEST-ACC_0.7259.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58944b91af4f4cee97b545bdb137bf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LSTM_attention_glove_add_words_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1:   0%|          | 0/10 [00:00<?, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd96a5460d4489fbafbd719cf5752cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7d8414c0b14e35a85ef641b42c98f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Train_loss: 0.7179, Train_acc: 0.7475 | Test_loss: 0.7526, Test_acc: 0.7327\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.7327.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b6111eb15c4dd8af56a1154648face",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121afe7e44de4c15bce66dbfe7267073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train_loss: 0.7083, Train_acc: 0.7496 | Test_loss: 0.7475, Test_acc: 0.7335\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1_EPOCH_1_TEST-ACC_0.7335.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52e61e335d342efa73677dba84f5049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0c50e6209c4081b6141dcee99005d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Train_loss: 0.7022, Train_acc: 0.7509 | Test_loss: 0.7472, Test_acc: 0.7367\n",
      "[INFO] Saving model to: ../models/LSTM_attention_glove_add_words_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1_EPOCH_2_TEST-ACC_0.7367.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28852c6f032c46e7ab82dc31dd4b9dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a365c8877424f8baf7c16c3d9454b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Train_loss: 0.7012, Train_acc: 0.7520 | Test_loss: 0.7480, Test_acc: 0.7346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8b0459eb734c3eba6e2d1776c42ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9bc9ef8ab64e29be7b033a0a13e3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Train_loss: 0.7001, Train_acc: 0.7535 | Test_loss: 0.7463, Test_acc: 0.7336\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f3f3f5b3eb4b64af9b211201bcbbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tglim/Kaggle/notebooks/discrimination.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m model_glove\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tuning_results \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mHP_tune_train(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_generator\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model_weights\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mLSTM_attention_glove_add_words_discrimination\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset_sub,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test_dataset\u001b[39m=\u001b[39;49mval_dataset_sub,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     class_names\u001b[39m=\u001b[39;49mclass_names,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     learning_rate_list\u001b[39m=\u001b[39;49mlearning_rate_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     weight_decay_list\u001b[39m=\u001b[39;49mweight_decay_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     epochs_list\u001b[39m=\u001b[39;49mepochs_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     batch_size_list\u001b[39m=\u001b[39;49mbatch_size_list,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     is_tensorboard_writer\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     gradient_accumulation_num\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     saving_max\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     metric_learning\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tglim/Kaggle/notebooks/discrimination.ipynb#X52sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:263\u001b[0m, in \u001b[0;36mHP_tune_train\u001b[0;34m(model, model_generator, model_weights, model_name, train_dataset, test_dataset, class_names, learning_rate_list, weight_decay_list, epochs_list, batch_size_list, is_tensorboard_writer, device, gradient_accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m is_tensorboard_writer:\n\u001b[1;32m    257\u001b[0m     writer \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcreate_writer(\n\u001b[1;32m    258\u001b[0m         experiment_name\u001b[39m=\u001b[39m model_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_test\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m    260\u001b[0m         extra\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLR_\u001b[39m\u001b[39m{\u001b[39;00mlearning_rate\u001b[39m}\u001b[39;00m\u001b[39m_WD_\u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m_EP_\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m_BS_\u001b[39m\u001b[39m{\u001b[39;00mbatch_size\u001b[39m}\u001b[39;00m\u001b[39m_GA_\u001b[39m\u001b[39m{\u001b[39;00mgradient_accumulation_num\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    261\u001b[0m     )\n\u001b[0;32m--> 263\u001b[0m model_results \u001b[39m=\u001b[39m train_tensorboard_gradient_accumulation(\n\u001b[1;32m    264\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    265\u001b[0m     save_name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m_LR_\u001b[39;49m\u001b[39m{\u001b[39;49;00mlearning_rate\u001b[39m}\u001b[39;49;00m\u001b[39m_WD_\u001b[39;49m\u001b[39m{\u001b[39;49;00mweight_decay\u001b[39m}\u001b[39;49;00m\u001b[39m_BS_\u001b[39;49m\u001b[39m{\u001b[39;49;00mbatch_size\u001b[39m}\u001b[39;49;00m\u001b[39m_GA_\u001b[39;49m\u001b[39m{\u001b[39;49;00mgradient_accumulation_num\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    266\u001b[0m     train_dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    267\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m    268\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(),\n\u001b[1;32m    269\u001b[0m     optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(\n\u001b[1;32m    270\u001b[0m         params\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mparameters(),\n\u001b[1;32m    271\u001b[0m         lr\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    272\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mweight_decay\n\u001b[1;32m    273\u001b[0m     ),\n\u001b[1;32m    274\u001b[0m     accuracy_fn\u001b[39m=\u001b[39;49mAccuracy(\n\u001b[1;32m    275\u001b[0m         task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmulticlass\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m    276\u001b[0m         num_classes\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(class_names)\n\u001b[1;32m    277\u001b[0m     ),\n\u001b[1;32m    278\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    279\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    280\u001b[0m     writer\u001b[39m=\u001b[39;49mwriter,\n\u001b[1;32m    281\u001b[0m     accumulation_num\u001b[39m=\u001b[39;49mgradient_accumulation_num,\n\u001b[1;32m    282\u001b[0m     saving_max\u001b[39m=\u001b[39;49msaving_max,\n\u001b[1;32m    283\u001b[0m     metric_learning\u001b[39m=\u001b[39;49mmetric_learning\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    286\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    287\u001b[0m t_result[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m model_results[\u001b[39m'\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:154\u001b[0m, in \u001b[0;36mtrain_tensorboard_gradient_accumulation\u001b[0;34m(model, save_name, train_dataloader, test_dataloader, loss_fn, optimizer, accuracy_fn, epochs, device, writer, accumulation_num, saving_max, metric_learning)\u001b[0m\n\u001b[1;32m    151\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    153\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(epochs), desc\u001b[39m=\u001b[39msave_name, leave\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 154\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step_gradient_accumulation(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    155\u001b[0m                                        dataloader\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m    156\u001b[0m                                        loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m    157\u001b[0m                                        optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m    158\u001b[0m                                        accuracy_fn\u001b[39m=\u001b[39;49maccuracy_fn,\n\u001b[1;32m    159\u001b[0m                                        device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    160\u001b[0m                                        accumulation_num\u001b[39m=\u001b[39;49maccumulation_num,\n\u001b[1;32m    161\u001b[0m                                        metric_learning\u001b[39m=\u001b[39;49mmetric_learning)\n\u001b[1;32m    162\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    163\u001b[0m                                     dataloader\u001b[39m=\u001b[39mtest_dataloader,\n\u001b[1;32m    164\u001b[0m                                     loss_fn\u001b[39m=\u001b[39mloss_fn,\n\u001b[1;32m    165\u001b[0m                                     accuracy_fn\u001b[39m=\u001b[39maccuracy_fn,\n\u001b[1;32m    166\u001b[0m                                     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    167\u001b[0m                                     metric_learning\u001b[39m=\u001b[39mmetric_learning)\n\u001b[1;32m    169\u001b[0m     results[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(train_loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/Kaggle/notebooks/../python_scripts/engine.py:38\u001b[0m, in \u001b[0;36mtrain_step_gradient_accumulation\u001b[0;34m(model, dataloader, loss_fn, optimizer, accuracy_fn, device, accumulation_num, metric_learning)\u001b[0m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m X_batch_train, y_batch_train \u001b[39min\u001b[39;00m tqdm_notebook(dataloader, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 38\u001b[0m   X_batch_train, y_batch_train \u001b[39m=\u001b[39m X_batch_train\u001b[39m.\u001b[39;49mto(device), y_batch_train\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m   \u001b[39mif\u001b[39;00m metric_learning:\n\u001b[1;32m     41\u001b[0m     y_logits \u001b[39m=\u001b[39m model(X_batch_train, y_batch_train)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = model_glove\n",
    "\n",
    "tuning_results = engine.HP_tune_train(\n",
    "    model=model,\n",
    "    model_generator=None,\n",
    "    model_weights=None,\n",
    "    model_name='LSTM_attention_glove_add_words_discrimination',\n",
    "    train_dataset=train_dataset_sub,\n",
    "    test_dataset=val_dataset_sub,\n",
    "    class_names=class_names,\n",
    "    learning_rate_list=learning_rate_list,\n",
    "    weight_decay_list=weight_decay_list,\n",
    "    epochs_list=epochs_list,\n",
    "    batch_size_list=batch_size_list,\n",
    "    is_tensorboard_writer=False,\n",
    "    device=device,\n",
    "    gradient_accumulation_num=1,\n",
    "    saving_max=True,\n",
    "    metric_learning=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_csv('../data/Discrimination/test.csv')\n",
    "\n",
    "# tt = []\n",
    "# for i in tqdm_notebook(range(len(train_data))):\n",
    "#     tt.append(len(preprocess_korean_text(train_data.iloc[i]['text'])))\n",
    "\n",
    "# plt.boxplot(tt)\n",
    "# print(max(tt), min(tt), np.mean(tt), np.var(tt))\n",
    "# print(np.sum(np.array(tt) > 100), '/', len(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_weight = torch.load('..\\models\\discrimination\\LSTM_attention_glove_discrimination_LR_0.0001_WD_0.0001_BS_64_GA_1_EPOCH_0_TEST-ACC_0.7751.pth')\n",
    "model.load_state_dict(loaded_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../data/Discrimination/test.csv')\n",
    "labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "  for i in tqdm_notebook(range(len(test_data))):\n",
    "    test_text = test_data.loc[i, \"text\"]\n",
    "    test_tokens = preprocess_korean_text(test_text)\n",
    "    if len(test_tokens) > max_length:\n",
    "        test_tokens = test_tokens[:max_length]\n",
    "    else:\n",
    "        test_tokens += [\"\"] * (max_length - len(test_tokens))\n",
    "\n",
    "    indices = []\n",
    "    for token in test_tokens:\n",
    "      if token in embed_model.wv:\n",
    "        indices.append(embed_model.wv.key_to_index[token])\n",
    "      else:\n",
    "        indices.append(embed_model.wv.key_to_index['<unk>'])  # use the index of the <unk> token for out-of-vocabulary words\n",
    "\n",
    "    test_logits = model(torch.tensor(indices).unsqueeze(0).to(device))\n",
    "    labels.append(class_names[torch.argmax(test_logits.squeeze(0).cpu())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['label'] = [idx_to_class[label] for label in labels]\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.DataFrame({'ID': range(len(test_data)), 'label': labels})\n",
    "submission_data.to_csv('../submissions/discrimination/submission.csv', index=False)\n",
    "print('submission completed!')\n",
    "submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
